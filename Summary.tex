\chapter{Summary and concluding remarks}
	\label{CH_summary}

In this work, we have presented a dynamic modal logic for reasoning about the relationship between an agent's beliefs, knowledge, and action, with a distinction between mere action and safe action expressible. We named this logic Dynamic Agent Safety Logic, or \DASL. Including this many modalities in one logic makes it very expressible and suitable for modelling a variety of realistic situations involving human-like agents. Additionally, the logic was developed with careful concern for realism. This departs from the typical thesis of modern modal ogic, which develops a logic with advanced formal properties without much regard for the realism of its foundational axiom schemas. By carefully considering the philosophical implications of our static base, the thesis made advances in reconnecting the worlds of epistemology and epistemic logic, and philosophy and modal logic more generally.

We paid careful attention to the philosophical commitments our epistemic logic carried with it, and worked to develop a philosophically defensible static foundation. This required us to adopt philosophically defensible positions against skepticism and of justification, which we did. We specified an interpretation of the possibility relations that conforms well to the predominant epistemic theory known as evidentialism, with a foundationalist structure to justification. In this sense, the thesis makes advances as a formal theory of epistemology, though it is certainly not complete. More work needs to be done in connecting the formalized interpretations to the system itself. One idea for doing this is to build a simple AI system with sensors and actuators, which abides by the axioms of \DASL\ in its reasoning, and constructs models based on objective and subjectively available evidence. This is beyond the scope of the thesis, however.

As a thesis of computer science and logic, we took care to use computational tools to validate our logical theses. We used the Coq Proof Assistant to mechanically check our proofs of soundness and completeness to a large degree, and similarly mechanized our case studies which validate \DASL's application to aviation safety. 

The proof of completeness mechanized a powerful aspect of modal logic from Sahlqvist and van Benthem. This dissertation, as far as we know, marks the first use of Coq for proving a modal logic's completeness via Sahlqvist theorems. To do this, we developed inductive predicates over the structure of axiom schemas corresponding to the definitions in Blackburn \etal\cite{modal}. This two-leveled approach to mechanization allowed us to prove theorems about the logic, and then drop down into the object language and prove theorems in the logic. This approach is extensible to other domains, where an object language with different atoms could be defined in Coq, while the upper level schema mechanization remains fixed, along with its results.

We mechanically checked our formalization of three cases of aviation mishaps, zeroing in on particular moments from the events where \DASL\ enables an inference from action to ignorance about safety-critical information. This inference was foreshadowed by the foundational models of agency in classical game theory, which infer good actions from assumptions of rationality and knowledge. A boolean manipulation of that classical inference yields the inference from bad action and assumption of rationality to an absence of knowledge. \DASL\ provides the formal logic for capturing this inference in a rich and realistic fashion.

Finally, we confronted a formal problem facing epistemic logics with self-reflective agents. The problem is related to L\"ob's Theorem in mathematical logic, formalized in provability logic by Boolos, and named L\"ob's Obstacle by researchers in formal agent foundations. As noted by Smullyan, self-reflective agents believing things about themselves are formally identical to mathematical systems, \emph{e.g.} Peano Arithmetic, proving statements about themselves. For contexts in which self-reflective agents should be explicitly modeled, as in AI safety research or realistic models of human reasoning, L\"ob's Obstacle must be addressed. We show that the static foundation for \DASL, in addition to the other dominant epistmic logics, suffer from L\"ob's Obstacle, and are therefore inadequate. We respond by weakening \DASL's base to \DASL$^-$, and retain all of \DASL's strengths but the philosophically desirable feature of indistinguishibility for the possibility relations. Further work must be done to explore this weakened \DASL\ and alternative ways to avoid L\"ob's Obstacle, but they are beyond the scope of this thesis. 