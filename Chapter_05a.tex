\chapter{L\"ob's Obstacle}
	\label{CH_05}

Thus far, the dynamic modal logic we have constructed and applied has followed closely the standard formalizations in the literature, departing primarily in how it deals with an agent's static knowledge. The static operator for an agent's belief remains $\mathit{KD}45$, as it is normally formalized in the literature. Our dynamic operators follow closely those defined in \cite{DEL} and \cite{PAL}, with the novel addition of a safety precondition to capture normative constraints on action in addition to logical constraints. The dissertation thus far, and the logics of knowledge and agency in the literature, ignore a problem facing agents of a certain reflective type, identified by Smullyan in \cite{smullyan}.\footnote{Smullyan refers to these as reflexive reasoners, but we use the term `reflective' in order to avoid ambiguity with the reflexive frame condition on worlds.} This chapter confronts the problem of reflective reasoners facing modal logics of knowledge and agency, including \DASL. We respond with an adjustment to \DASL's static foundation so that the problem can be avoided, if the agents being modeled are reflective reasoners.

In Smullyan's, ``Logicians who reason about themselves," he considers epistemic problems related to undecidability results in mathematics. He identifies ``a complete parallelism between logicians who believe propositions and mathematical systems that prove propositions." In provability logic, the formula $\varphi \iiff \tlnot \Bels{i}\varphi$ expresses the G\"odel proposition, ``This proposition is not provable in system $i$." In a doxastic interpretation, the same formula expresses the reflective belief, ``agent $i$ does not believe this proposition." This means that for any doxastic or epistemic system, if the agents it models are reflective, then it must not have the theorem $\Bels{i}\varphi \iimplies \varphi$, under pain of contradiction. 

A \emph{reflective} agent is one that can form beliefs and knowledge about self-referential sentences and propositions. Such propositions are of the form $\varphi \iiff (\Bels{i}\varphi \iimplies \psi)$. The reflective proposition ``agent $i$ does not believe this proposition" is of that form, where $\psi$ is replaced with $\bot$ in the generic form. More specifically, an agent is \emph{reflective} just in case for every proposition $\psi$, there is a proposition $\varphi$ such that $\varphi \iimplies (\Bels{i}\varphi \iimplies \psi)$.

The above uses the $\Bels{i}$ operator for belief, but just as easily we could have used $\Kns{i}$. The problem for epistemic logic is immediately apparent. Most humans are capable of reasoning about self-referential propositions, so a logic for human knowledge ought to include such propositions. However, with the Truth Axiom of the knowledge operator, this seems to yield inconsistency. It seems to yield the very same inconsistency that mathematical system $i$ would face if it could prove its own soundness.

In what follows, we examine the mathematical logic that underpins this issue. Just as Smullyan identified the problem as one facing reflective reasoners with Axioms K and 4 and the Rule of Necessitation in \cite{smullyan}, L\"ob identified these same conditions as the ones that allow a mathematical system to derive his theorem in \cite{Lob}. We highlight the obstacle this presents to epistemic and doxastic logics, and show how to avoid it. We present a relaxed \DASL\ for reasoning about safety-critical information flow to reflective agents.

%The work, however, is not finished. This dissertation made advances along these lines, but did not fully explore the new terrain. We took care to establish a static base logic with epistemic and doxastic operators that was realistic, and in particular distinguished our static base from those of \SFive\ and $\mathcal{S}4$, as seen in much of the rest of the epistemic logic literature. However, these logics, and our own static base logic, potentially suffer from a serious problem not explored in the present work.

The potential problem is due to L\"ob in \cite{Lob} from the mathematical logic perspective, and Smullyan in \cite{smullyan} from the epistemic/doxastic logic perspective. Contemporary research in artificial intelligence foundations addresses the problem as it pertains to agents being confident in their own conclusions. See for example research at the Machine Intelligence Research Institute\footnote{See their website at https://www.intelligence.org}, who kindly invited the author to a workshop introducing him to the problem. Some of their preliminary results on the problem are available on the footnoted website. They have named the problem L\"ob's Obstacle, or the L\"obian Obstacle. We describe it, and the problem it presents to modal logic of agency, including our own, below.

\section{L\"ob's Theorem}
\label{sec:lob_section}
L\"ob's Theorem takes its name from Martin Hugo L\"ob, who tackled a question of mathematical logic posed by Leon Henkin in the years following the results of G\"odel. Henkin asked what could be said of propositions asserting their own provability, as opposed to unprovability in the case of G\"odel sentences.\cite{sep_prov_log} L\"ob answered by showing that in a consistent system, proof that proof of a proposition implies that the proposition is true (soundness) only for propositions that are actually provable. This is surprising, because it generalizes G\"odel's Second Incompleteness Theorem: no formal system can prove its own soundness (as opposed to consistency).

L\"ob's Theorem in provability logic is,
\begin{center}~\label{lob}
	\begin{equation}
	\Box(\Box\varphi \iimplies \varphi)\iimplies \Box\varphi.
	\end{equation}
\end{center}
The $\Box$ is interpreted as ``provability" in some formal system, particularly one at least as powerful as Peano arithmetic. If a modal operator involves the reasoning abilities of a human-like agent, then \emph{a fortiori} it is a formal system at least as powerful as Peano arithmetic. This presents the following problem. If that agent, in its own reasoning system, can deduce the soundness of its own system, then its reasoning system is unsound. This is because $\varphi$ can be any formula, including $\bot$. The particular modal logic at risk, in our mind, is epistemic logic, or any doxastic logic with accurate reasoners.

L\"ob identified the following conditions of a formal system that would allow derivation of his theorem (which we present in modal form).
\begin{enumerate}
	\item $\Box(\varphi \iimplies \psi) \iimplies (\Box\varphi \iimplies \Box \psi)$\mbox{}\hfill Axiom K
	\item $\Box\varphi \iimplies \Box\Box\varphi$ \mbox{}\hfill Axiom 4
	\item From $\vdash \varphi$, infer $\Box\varphi$ \mbox{}\hfill Rule of Necessitation
\end{enumerate}

Items (1) and (3) are constants for all normal modal logics. There is a suppressed condition that L\"ob did not mention because it was unnecessary for the domain of arithmetic, but we must mention it here. The system must either admit of self-referential sentences or involve modal fixed points. Because systems of human-like reasoning must include self-referential sentences, at least in the form of Peano arithmetic, in order to remain human-like, this condition is satisfied for our concerns. 

Here we give a template derivation of L\"ob's Theorem, which we shall refer to below when describing how L\"ob's Obstacle corrupts various epistemic logics.
\begin{proof}
	$\\$
\begin{proofenum}
	\item $\Box(\Box\varphi \iimplies \varphi)$\mbox{}\dotfill Assumption
	\item $\Box(\psi \iiff (\Box\psi\iimplies \varphi))$\mbox{}\dotfill L\"ob Sentence\footnote{Sometimes referred as a Curry sentence after logician Haskell Curry.}
	\item $\Box(\Box\psi \iiff \Box(\Box\psi \iimplies \varphi))$\mbox{}\dotfill Axiom K
	\item $\Box(\Box\psi \iimplies \Box(\Box\psi \iimplies \varphi)))$\mbox{}\dotfill (1.3) Simplification of $\iiff$
	\item $\Box(\Box\psi \iimplies (\Box\Box\psi \iimplies \Box\varphi))$\mbox{}\dotfill (1.4) Axiom K
	\item $\Box(\Box\psi \iimplies \Box\Box\psi)$\mbox{}\dotfill Axiom 4
	\item $\Box(\Box\psi \iimplies \Box\varphi)$\mbox{}\dotfill (1.5), (1.6)
	\item $\Box(\Box\psi \iimplies \varphi)$\mbox{}\dotfill (1.7), (1.1)
	\item $\Box\psi$\mbox{}\dotfill (1.3), (1.8)
	\item $\Box\Box\psi$\mbox{}\dotfill (1.9), Axiom 4
	\item $\Box\Box\psi \iimplies \Box\varphi$\mbox{}\dotfill (1.8), Axiom K
	\item $\Box\varphi$\mbox{}\dotfill (1.10), (1.11)
\end{proofenum}\mbox{}\hfill $\mathcal{QED}$
\end{proof}

Mathematical and Provability logicians refer to the key components of this proof as L\"ob Conditions\cite{Boolos}. Identifying them in the proof above helps us identify which epistemic logics collide with L\"ob's Obstacle. Conversely, understanding how the L\"ob Conditions interact helps us construct epistemic logics that avoid the Obstacle.

The Conditions are:
\begin{enumerate}
	\item The L\"ob Sentence. A self-referential sentence, also formalizable as a modal fixed point.
	\item Axiom K. The standard distribution axiom of normal modal logics.
	\item Axiom 4. The axiom corresponding to a transitive frame relation.
	\item The rule of necessitation. Likewise a standard feature of normal modal logics.
\end{enumerate} 

The L\"ob Sentence is sometimes not mentioned as a Condition, because L\"ob's Theorem is typically studied in the context of mathematical logic or provability logic, where such self-referential expressiveness is known to exist. We point out, however, that humans are capable of reasoning about self-referential sentences, as well, and any advanced artificial agent will be able to do so, as well. 

Finally, we note the importance of L\"ob's Theorem's antecedent: $\Box(\Box\varphi \iimplies \varphi)$. Epistemic logics typically include the antecedent as a theorem, in which case L\"ob's Theorem will allow us to derive $\Box\varphi$ for all $\varphi$. This is why consistent mathematical systems at least as expressive as Peano arithmetic cannot prove their own consistency. 

In what follows, we summarize the problems this theorem presents to formal agency modeling in general, and a unique potential benefit it affords in game theory due to LaVictoire \etal in \cite{MIRIPD}. Next, we round up the usual suspects of epistemic-doxastic logic and show that, on the assumption that they aim to capture human-like reasoning, they crash into L\"ob's Obstacle. Unfortunately, \DASL\ itself suffers from L\"ob's Obstacle, and so we provide a weakened static base suitable for agents capable of self-reflective reasoning. It sustains the contraposed game theoretic inference we sought out to model for realistic agents.

\section{Discussion on L\"ob's Obstacle}
\label{sec:lob_discussion}

L\"ob's Obstacle has its origin in mathematical logic, where it was shown to prevent Peano arithmetic from proving its own soundness. Among modal logicians of a philosophical bent, mathematical logic, and its provability logic cousin, have received much less interest than epistemic, temporal, and deontic logics. Among modal logicians interested in technical aspects of logic, L\"ob's Theorem has received interest insofar as it relates to fixpoints. One exception is Raymond Smullyan in \cite{smullyan}, where he examined the parallels between Peano Arithmetic$^+$\footnote{We use this to refer to any system at least as expressive as PA.} proving statements about itself and logicians deducing beliefs about themselves. In it, he points out that reasoners with certain powers will risk deviating into an inconsistent set of beliefs, due to self-reflection. The cited paper appeared as a curiosity of sorts, presenting a series of recreational logic puzzles or riddles, with corresponding solutions following. 

More recently, researchers in formal agent modeling have become interested in this particular logic puzzle of Smullyan's. When the logician reasoning about itself is an artificial intelligence, the puzzle becomes a unique safety problem. In \cite{yudkowski}, Yudkowski and Herreshoff consider an AI of sufficient reasoning power (PA$^+$), and show by straightforward application of L\"ob's Theorem that such an agent can neither trust its own conclusions in general, nor assent to its, we would say \emph{in fact}, correct reasoning principles. The alternative to this is an inconsistent agent.

According to Yudkowski and Herreshoff, the AI ``cannot say, `I don’t know if $\varphi$ is true, but I trust that if any proof exists from my axioms of $\varphi$, then $\varphi$ must be true.' Instead [the AI] can only react to particular proofs of $\varphi$ by asserting $\varphi$. This presents an apparent paradox of rational coherence$\dots$ the agent will believe any proof of $\varphi$ you present to it, but will refuse to assent to `I ought to believe any proof of $\varphi$ I am presented with,' a seeming defect of reflectivity $-$ the agent cannot approve, under reflection, its own behavioral principles." 

The safety problem is an instance of $\varphi$ in the previous paragraph. One can substitute any safety property in for $\varphi$ that a future AI ought to abide by. We can ignore problems of humans correctly encoding the safety property, and focus on the AI assessing actions under the constraint of $\varphi$. We assume some such actions will be of the meta variety, where it must decide whether to $a)$ modify itself, $b)$ create a copy of itself, or $c)$ produce offspring AI. Any such meta type action causes it to confront a sentence of the form, ``[meta type action $\laa$] does not violate $\varphi$." If the AI is producing strictly weaker reasoning systems as offspring or self-modifications, it can safely infer that the sentence is true\cite{yudkowski}. This can produce a descending chain of systems culminating in PA. However, for self-copies and non-weakening self-modifications, the AI cannot infer the above sentence without risking inconsistency.

Perhaps for the short-term, this problem can be avoided due to the fact that powerful AI programs tend to be \emph{narrow}, or focused on a particular problem domain. So long as AI programs tend to be narrow, formal methods researchers may be able to specify safety properties and develop techniques for mechanically checking whether they hold. However, there are three reasons to be skeptical of this optimistic claim.

First, this avoids the technical problem by assuming it will not arise, not by solving it. Given a technical scenario with potentially unsafe consequences, proper safety engineering requires examination of worst case scenarios and engaging with them.

Second, one instance of narrow AI is a program focused on constructing software. This scenario seems highly likely to occur in the relatively near future, as more technology development abstracts away from and automates various levels of configuration and compilation. Such automation trends toward a program that takes as input a computational problem and attempts to construct a program that solves it. Due to the Curry-Howard Correspondence between proofs and programs, this type of program in fact already exists in the form of automated theorem provers. Should these programs continue to advance to a point where they are constructing other programming engines, the problem of L\"ob's Obstacle immediately arises, even for narrow AI.

Finally, organizations are already actively working to develop artificial \emph{general} intelligence, defined as an AI capable of solving problems from any domain at least as well as a human. A company which has made numerous headlines for its recent successes is DeepMind, a subsidiary of Alphabet, which states that its mission is, ``We’re on a scientific mission to push the boundaries of AI, developing programs that can learn to solve any complex problem without needing to be taught how."\footnote{https://deepmind.com/about/ Accessed 7/9/19.} With a strong economic incentive to achieve artificial general intelligence, it is not unreasonable to engage with the safety concerns that arise from it \emph{prior} to its successful creation.

The consequences of L\"ob's Theorem for AI safety are as follows. First, ignoring chains of descendingly weaker systems, AI cannot safely be in the AI programming business. There is a sort of mathematical gravity that pulls a system toward L\"ob's Obstacle, as it places two clear desiderata in conflict with one another. On the one hand, we seek agents with (near-) perfect \emph{external} accuracy, with beliefs that are justifiedly true of the world. On the other hand, we seek agents with (near-) perfect \emph{internal} accuracy, with powerful introspective access to their own internal states. Combining these desiderata yields an \SFive\ modality, and indeed, the ideally rational agent is one with \SFive\ epistemic modality. Aumann's formalization of the ideally rational agents of game theory and economics is the \SFive\ modal operator in \cite{Aumann}. His was not an arbitrary formalization, but rather one that made explicit assumptions that mathematical models in those domains make about agents. Thus, there is a tendency, in constructing artificial agents, to build toward that. Such agents have the powerful positive and negative introspection properties, and the axiom that knowledge implies truth.

However, constructing such an agent capable of reasoning about PA$^{+}$ is guaranteed to eventually decay into inconsistency, by L\"ob's Theorem, as pointed out by Smullyan. Such agents satisfy the antecedent of L\"ob's Theorem, and are able to deduce L\"ob's Theorem itself, and therefore conclude that they know any arbitrary formula. The problematic case here is not that the AI will come to believe that apples are blue, but rather that producing a next generation AI$^+$ is provably safe, when this is not the case.

As this problem relates to the goal of this thesis, which is to describe a logic of agency for human-like agents, we examine some static epistemic logics that crash into L\"ob's Obstacle.



\section{Epistemic Logics that Crash}
\label{sec:crashing_logics}
\subsection{\SFive\ Epistemic Logic}
The most prominent epistemic logic in the literature, by far, is \SFive\ epistemic logic. \SFive\ epistemic logic is routinely presented as the logic of knowledge, and often serves as a static base for dynamic extensions to epistemic logic involving action and communication. Its characteristic axioms are:
\begin{eqnarray}
\Kns{i}(\varphi \iimplies \psi)\iimplies (\Kns{i}\varphi \iimplies \Kns{i}\psi)\\
\Kns{i}\varphi \iimplies \varphi\\
\tlnot\Kns{i}\varphi \iimplies \Kns{i}\tlnot\Kns{i}\varphi
\end{eqnarray}
Clearly (5.2) is Axiom K, and (5.3), troublingly, combines with the Rule of Necessitation to produce the antecedent of L\"ob's Theorem. (5.4) is called the Negative Introspection axiom, or sometimes in philosophy circles, the Wisdom Axiom. Logicians call it Axiom 5. It is read, ``If $i$ does not know that $\varphi$, then she knows that she doesn't know it". Other than being clearly invalid for humans, this axiom and (3) allows us to derive,
\begin{equation*}
\Kns{i}\varphi \iimplies \Kns{i}\Kns{i}\varphi
\end{equation*}

\begin{proof}
	$\\$
	\begin{proofenum}
		\item $\tlnot\Kns{i}\tlnot\Kns{i}\varphi \iimplies \Kns{i}\varphi$\mbox{}\hfill Contrapositive of Axiom 5
		\item $\Kns{i}\tlnot\Kns{i}\tlnot\Kns{i}\varphi \iimplies \Kns{i}\Kns{i}\varphi$\mbox{}\hfill Rule of Necessitation on (5.1), Axiom K
		\item $\varphi \iimplies \tlnot\Kns{i}\tlnot \varphi$\mbox{}\hfill Axiom T, Contrapositive
		\item $\tlnot\Kns{i}\tlnot\varphi \iimplies \Kns{i}\tlnot\Kns{i}\tlnot\varphi$\mbox{}\hfill Axiom 5
		\item $\varphi \iimplies \Kns{i}\tlnot\Kns{i}\tlnot \varphi$\mbox{}\hfill (4.3), (4.4)
		\item $\Kns{i}\varphi \iimplies \Kns{i}\tlnot\Kns{i}\tlnot\Kns{i}\varphi$\mbox{}\hfill $\Kns{i}\varphi$/$\varphi$, (4.5)
		\item $\Kns{i}\varphi\iimplies\Kns{i}\Kns{i}\varphi$\mbox{}\hfill (4.2), (4.6)
		
	\end{proofenum}\mbox{}\hfill$\mathcal{QED}$
\end{proof}

Thus, \SFive\ satisfies L\"ob's three conditions, if we assume the presence of self-referential sentences possible, which we should. Therefore, with $\Kns{i}$ instead of $\Box$, the proof of L\"ob's Theorem is possible in this brand of \SFive. However, to make matters worse, the antecedent of L\"ob's Theorem is itself an axiom of S5. Therefore, $\Kns{i}\varphi$ is a theorem, for all $\varphi$.

We take this as a \emph{reductio ad absurdum} that \SFive\ epistemic logic cannot be a logic for reasoning about the knowledge of agents with expressive power beyond Peano arithmetic. Therefore, it cannot be a logic of knowledge for humans, or human-like agents.

\subsection{Hintikka's S4 Epistemic Logic}
\label{sec:hint_s4}
In Hintikka's 1967 \emph{Knowledge and Belief: A logic of the two notions}, he presented an epistemic logic for determining the validity and consistency of claims people make about knowledge and belief. He rejected out of hand the negative introspection axiom for knowledge, but chose to include positive introspection, which is formalized as $\Kns{i}\varphi\iimplies\Kns{i}\Kns{i}\varphi$. Clearly then, if Hintikka's epistemic system is meant for human-like reasoners who can express sentences like, ``If I know this sentence is true, then 1 + 1 = 2," then it crashes into L\"ob's Obstacle, with the extra bite of having the antecedent of L\"ob's Theorem as a theorem itself, and therefore, $\Kns{i}\varphi$ is also a theorem.

\subsection{Kraus and Lehmann System}
\label{sec:kl}

In \cite{KrausLehmann}, Kraus and Lehmann tackle the issue of how to combine knowledge and belief in a single system remarked on in the conclusion of Halpern and Moses in \cite{HalpernMoses}. The project at the time was very similar to that addressed by this dissertation's static foundation: the development of a logic sufficiently realistic and expressive for reasoning about intelligent agents. 

They axiomatize knowledge and belief as follows.

\begin{table}[H]
	\begin{center}
		\begin{tabular}{| l r |}
			\hline
			$\Kns{i}(\varphi \iimplies \psi) \iimplies (\Kns{i}\varphi \iimplies \Kns{i}\psi)$ & Distribution of $\Kns{i}$ \\
			$\Kns{i}\varphi \iimplies \varphi$ & Truth \\
			$\tlnot\Kns{i}\varphi \iimplies \Kns{i}\tlnot\Kns{i}\varphi$ & Negative Introspection \\
			$\Bels{i}(\varphi \iimplies \psi) \iimplies (\Bels{i}\varphi \iimplies \Bels{i}\psi)$ & Distribution of $\Bels{i}$\\
			$\Bels{i}\varphi \iimplies \BPoss{i}\varphi$ & Belief Consistency \\
			%		$\Bels{i}\varphi \iimplies \Bels{i}\Bels{i}\varphi$ & Positive Belief Introspection \\
			%		$\tlnot\Bels{i}\varphi \iimplies \Bels{i}\tlnot\Bels{i}\varphi$ & Negative Belief Introspection\\
			$\Kns{i}\varphi \iimplies \Bels{i}\varphi$ & Knowledge implies Belief \\
			$\Bels{i}\varphi \iimplies \Kns{i}\Bels{i}\varphi$ & Conscious Belief\\
			%			$\Bels{i}\varphi \iimplies \Kns{i}\Bels{i}\varphi$ & Beliefs are Known\\
			From $\vdash \varphi$ and $\vdash \varphi \iimplies \psi$, infer $\vdash\psi$ & Modus Ponens\\
			From $\vdash \varphi$, infer $\vdash \Kns{i}\varphi$ & Necessitation of $\Kns{i}$\\
			\hline
		\end{tabular}
		\caption{Logic of Kraus and Lehmann}~\label{KL}
	\end{center}
\end{table}

The key differences with \DASL\ are that knowledge is \SFive\ and the axiom of Conscious Belief, which reverses the composition of knowledge and belief under belief relative to \DASL's Evidential Restraint axiom. From the above axiom schemas, it follows that belief is a regular $\mathcal{KD}45$ operator.

In their article, and in Meyer and van der Hoek's \cite{MeyerHoek}, they show that $\Bels{i}(\Bels{i}\varphi \iimplies \varphi)$ is a theorem. Therefore, this system suffers from a particularly bad collision with L\"ob: Both the knowledge and belief modalities lead to inconsistency for intelligent agents capable of self-referential reasoning. 

\section{\DASL}
\label{sec:crap}

\begin{table}[H]
	\begin{center}
		\begin{tabular}{| l r |}
			\hline
			$\Kns{i}(\varphi \iimplies \psi) \iimplies (\Kns{i}\varphi \iimplies \Kns{i}\psi)$ & Distribution of $\Kns{i}$ \\
			$\Kns{i}\varphi \iimplies \varphi$ & Truth \\
			$\Bels{i}(\varphi \iimplies \psi) \iimplies (\Bels{i}\varphi \iimplies \Bels{i}\psi)$ & Distribution of $\Bels{i}$\\
			$\Bels{i}\varphi \iimplies \BPoss{i}\varphi$ & Belief Consistency \\
			%		$\Bels{i}\varphi \iimplies \Bels{i}\Bels{i}\varphi$ & Positive Belief Introspection \\
			$\tlnot\Bels{i}\varphi \iimplies \Bels{i}\tlnot\Bels{i}\varphi$ & Negative Belief Introspection\\
			$\Kns{i}\varphi \iimplies \Bels{i}\varphi$ & Knowledge implies Belief \\
			$\Bels{i}\varphi \iimplies \Bels{i}\Kns{i}\varphi$ & Evidential Restraint\\
			%			$\Bels{i}\varphi \iimplies \Kns{i}\Bels{i}\varphi$ & Beliefs are Known\\
			From $\vdash \varphi$ and $\vdash \varphi \iimplies \psi$, infer $\vdash\psi$ & Modus Ponens\\
			From $\vdash \varphi$, infer $\vdash \Kns{i}\varphi$ & Necessitation of $\Kns{i}$\\
			\hline
		\end{tabular}
		\caption{Logic of \DASL}~\label{DASL-static}
	\end{center}
\end{table}

Recall that in \DASL's static logic, knowledge lacks both epistemic introspection properties, while belief has negative introspection and positive introspection. The modalities are combined via the Knowledge implies Belief Axiom and the Evidential Restraint Axiom. L\"ob's Theorem is derivable for the belief operator. We first show that the two composite axiom schemas are sufficient to derive positive introspection for belief.

\begin{theorem}[Positive Belief Introspection]~\label{belief_posint}
	$\Bels{i}\varphi \iimplies \Bels{i}\Bels{i}\varphi$
\end{theorem}
\begin{proof}
	$\\$
\begin{proofenum}
		\item $\Bels{i}\varphi \iimplies \Bels{i}\Kns{i}\varphi$\mbox{}\hfill ER Axiom
		\item $\Bels{i}\Kns{i}\varphi \iimplies \Bels{i}\Bels{i}\varphi$\mbox{}\hfill KiB Axiom + Necessitation of $\Bels{i}$\footnote{This rule can be derived from Necessitation of $\Kns{i}$ and KiB Axiom.}
		\item $\Bels{i}\varphi \iimplies \Bels{i}\Bels{i}\varphi$\mbox{}\hfill (3.1), (3.2)
	\end{proofenum}
\end{proof}\mbox{}\hfill $\mathcal{QED}$

With the three L\"ob Conditions (Axiom K, Axiom 4, and the Rule of Necessitation) satisfied for $\Bels{i}$, L\"ob's Theorem holds for belief. 
%
%This logic also avoids the disaster of L\"ob's Theorem for belief, because $\Bels{i}(\Bels{i}\varphi \iimplies \varphi)$ is no longer a theorem.
%
%\begin{figure}[H]
%	\begin{center}
%		\begin{tikzpicture}[->,>=stealth',shorten >=1pt,auto,node distance=3.4cm,
%		thick,base node/.style={circle,draw,minimum size=35pt}]
%		
%		\node[base node] (w) {\begin{tabular}{c}
%			$w:  p$ \\ $\BPoss{i}(\Bels{i}p \tland \tlnot p)$
%			\end{tabular}};
%		\node[base node] (v) [right of=w] {\begin{tabular}{c}
%			$v:  \tlnot p$  \\
%			$\Bels{i}p$
%			\end{tabular}};
%		\node[base node] (u) [right of=v] {$u:  p$};
%		\path[]
%		(w) edge node[above] {$\Rel{k,b}^i$} (v)
%		edge [loop left] node {$\Rel{k}^i$} (w)
%		(v) 
%		%		edge node[below] {} (w)
%		%	\draw [->] (v) edge[in=5,out=355,loop] node[right] {$R$} (v)
%		edge [<-, loop above] node {$\Rel{k}^i$} (v)
%		edge node[above] {$\Rel{k,b}^i$} (u)
%		(u)  edge [<-, loop right] node {$\Rel{k,b}^i$} (u);
%		\end{tikzpicture}
%	\end{center}
%	\caption{A counterexample to $\Bels{i}(\Bels{i}\varphi \iimplies \varphi)$.}~\label{not_conceited}
%\end{figure}
%
%Therefore, L\"ob's Theorem has the same effect for belief as it does for Peano arithmetic: $i$ believes that her belief is accurate only for propositions that she actually believes. Her belief in her own accuracy is not a general claim. In this light, its effect is mitigated, in the same way it is for arithmetic. Because L\"ob's antecedent is not valid for belief, $i$ is not what Smullyan calls a \emph{conceited} reasoner. With L\"ob's Theorem for belief, $i$ constitutes what Smullyan calls a \emph{modest} reasoner.
%
%How shall we interpret the $\BPoss{i}$ operator? Figure \ref{not_conceited} could be read ``it is consistent with what $i$ believes that $i$ falsely believes that $p$". It is somewhat standard to interpret knowledge and belief operators in this way: ``...consistent with what $i$ knows/believes".  We do not endorse this interpretation due to the smell of circularity hanging around it. We prefer the interpretation ``$i$ considers it possible that..." for $\BPoss{i}$ and ``$i$ has evidence that..." for $\Poss{i}$. 
%
%If we consider the contrapositive of L\"ob's Theorem for belief, it states $\tlnot \Bels{i} \varphi \iimplies \tlnot\Bels{i}(\Bels{i} \varphi \iimplies \varphi)$. Indeed, this is G\"odel's Second Incompleteness Theorem applied to belief: ``if $i$ does not believe $\varphi$, then $i$ does not believe that believing $\varphi$ implies its truth." If we substitute in $BPoss{i}$ for $\tlnot\Bels{i}\tlnot$ and push the negation into the implication, the consequent is $\BPoss{i}(\Bels{i}\varphi \tland \tlnot \varphi)$, which is the formula illustrated in Figure \ref{not_conceited}. We read this as ``$i$ considers it possible that she believe $p$ but $p$ be false. This is perhaps a bit strange. However, the interpretations we propose have the following benefit.
%
%Recall that the contrapositive of the Knowledge implies Belief axiom is $\BPoss{i}\varphi \iimplies \Poss{i}\varphi$. Our interpretation reads this as, ``$i$ considers $\varphi$ to be possible only if she has evidence that $\varphi$."  This may not be valid for how humans \emph{in fact} form beliefs, but it provides a normative dimension to the logic and some modest level of idealization. The agent's modeled by this logic are human-like, and maybe some particular humans actually abide by these axioms. We call these agents Grounded-Coherent agents, because their beliefs are grounded in evidence, and coherent because they avoid L\"ob's Obstacle.
%
%The belief operator narrowly avoids L\"ob's Obstacle. 
The Belief Consistency Axiom $\Bels{i}\varphi\iimplies\BPoss{i}\varphi$ is equivalent to $\tlnot(\Bels{i}\varphi \tland \Bels{i}\tlnot\varphi)$, which is furthmore equivalent to $\tlnot\Bels{i}(\varphi \tland \tlnot \varphi)$, which results in the following disaster.

\begin{theorem}[Consistency Disaster]~\label{no_bel_cons}
	If $\tlnot\Bels{i}(\varphi \tland \tlnot \varphi)$ and\\ $\Bels{i}(\Bels{i}\varphi \iimplies\varphi)\iimplies \Bels{i}\varphi$ are theorems, then $\Bels{i}(\varphi \tland \tlnot \varphi)$ is a theorem.
\end{theorem}
\begin{proof}$\\$
\begin{proofenum}
		\item $\tlnot\Bels{i}(\varphi \tland \tlnot \varphi)$\mbox{}\hfill Belief is Consistent
		\item $\Bels{i}(\varphi\tland\tlnot\varphi)\iimplies (\varphi \tland \tlnot \varphi)$\mbox{}\hfill (4.1)
		\item $\Bels{i}(\Bels{i}(\varphi \tland \tlnot \varphi)\iimplies (\varphi \tland \tlnot \varphi))$\mbox{}\hfill Necessitation of $\Bels{i}$
		\item $\Bels{i}(\Bels{i}(\varphi \tland \tlnot \varphi) \iimplies (\varphi \tland \tlnot \varphi))\iimplies \Bels{i}(\varphi \tland \tlnot \varphi)$\mbox{}\hfill L\"ob's Theorem
		\item $\Bels{i}(\varphi \tland \tlnot \varphi)$\mbox{}\hfill (4.3), (4.4)
	\end{proofenum}
\end{proof}

Thus, with Belief Consistency, L\"ob's Theorem, and Theorem \ref{no_bel_cons}, it follows that:

\begin{theorem}[Beliefs Inconsistent]~\label{crap}
	For all $\varphi$, $\Bels{i}\varphi$.
\end{theorem}

\begin{proof}
	This follows from Theorem \ref{no_bel_cons} and $\tlnot\Bels{i}(\varphi \tland \tlnot \varphi)$.
\end{proof}\mbox{}\hfill $\mathcal{QED}$

Note the similarity here to the problem facing PA$^+$ were it to prove its own consistency, due to G\"odel's Second Incompleteness Theorem.\cite{godel}

We briefly note that the logic defined by these axiom schemas and inference rules avoids L\"ob's Obstacle for $\Kns{i}$, as it is no longer has the positive introspection property.

\begin{figure}[H]
	\begin{center}
		\begin{tikzpicture}[->,>=stealth',shorten >=1pt,auto,node distance=3cm,
		thick,base node/.style={circle,draw,minimum size=35pt}]
		
		\node[base node] (w) {\begin{tabular}{c}
			$w:  p$ \\ $\Kns{i}p$ \\ $\tlnot\Kns{i}\Kns{i}p$
			\end{tabular}};
		\node[base node] (v) [right of=w] {\begin{tabular}{c}
			$v:  p$ \\ $\tlnot\Kns{i}p $
			\end{tabular}};
		\node[base node] (u) [right of=v] {$u: \tlnot p$};
		\path[]
		(w) edge node[above] {$\Rel{k}^i$} (v)
		edge [loop left] node {$\Rel{k,b}^i$} (w)
		(v) 
		%		edge node[below] {} (w)
		%	\draw [->] (v) edge[in=5,out=355,loop] node[right] {$R$} (v)
		edge [<-, loop above] node {$\Rel{k}^i$} (v)
		edge node[above] {$\Rel{k}^i$} (u)
		(u)  edge [<-, loop right] node {$\Rel{k}^i$} (u);
		\end{tikzpicture}
	\end{center}
	\caption{A counterexample to $\Kns{i}\varphi\iimplies\Kns{i}\Kns{i}\varphi$.}
\end{figure}

Absent one of the three L\"ob Conditions, L\"ob's Theorem cannot be derived. Thus, we can say that the $\Kns{i}$ operator is L\"ob-safe, which is to say that the modal logic System T is L\"ob-safe. However, System T on its own is not adequate to model the knowledge and belief of human-like agents. 
%However, so long as it is merely not the case that $i$ believes $\varphi$ and that she believes $\tlnot \varphi$, we avoid disaster. It is worth exploring the cost of avoiding L\"ob's Obstacle. Does this epistemic logic still characterize something resembling a realistic logic of human-like knowledge? It still suffers from the problem of logical omniscience, for both knowledge and belief. This is unavoidable for a normal modal logic. It maintains the critical axiom that knowledge is true. 

One might wonder whether it would be acceptable to abandon the Truth Axiom for knowledge and allow L\"ob's Theorem to hold for the knowledge operator in a way that avoids inconsistency. This would introduce more modesty to the notion of knowledge, where a human-like agent knows that her knowledge is true only for those propositions that she actually knows, but not in the general sense.\footnote{Smullyan referred to such agents as modest agents, for they are confident in the accuracy only of particular beliefs that they have good reasons (proofs) for, but lack a general confidence in their own beliefs.} 

What would this mean for epistemology? A false proposition would no longer imply a lack of knowledge, and the rejection of the truth axiom goes against the entire history of Western philosophical thought. This is to say, it would require robust philosophical defense, which we are not prepared to give here. Relaxing the Truth Axiom allows positive and negative introspection to live harmoniously with self-reference. We leave this for future work to explore. We note here, however, that work by Barasz \etal in \cite{modal_prisoner} show that agents with L\"ob's Theorem holding for their epistemic operators are able to cooperate in the Prisoner's Dilemma game.\footnote{Specifically, they are programs capable of inspecting their own source code.} Related research in formal agent modeling via programs that play games with each other and can examine each other's source code has been explored by Binmore, Howard, McAfee, Tennenholtz, and other game theorists.

Rather than pursuing that route, we relax our axiom schema so as to avoid the derivation of L\"ob's Theorem entirely, while maintaining the same or similar realism sought by the original \DASL\ static schemas.

\section{Avoiding L\"ob}
\label{sec:avoiding_lob}
We leave the reader with the following logic that avoids the L\"obian Obstacle while retaining the inference from unsafe action to information about missing knowledge, the useful and interesting contrapositive to the standard game theoretic inference mentioned at the beginning of this thesis.

\begin{table}[H]
	\begin{center}
		\begin{tabular}{| l r |}
			\hline
			$\Kns{i}(\varphi \iimplies \psi) \iimplies (\Kns{i}\varphi \iimplies \Kns{i}\psi)$ & Distribution of $\Kns{i}$ \\
			$\Kns{i}\varphi \iimplies \varphi$ & Truth \\
			$\Bels{i}(\varphi \iimplies \psi) \iimplies (\Bels{i}\varphi \iimplies \Bels{i}\psi)$ & Distribution of $\Bels{i}$\\
			$\Bels{i}\varphi \iimplies \BPoss{i}\varphi$ & Belief Consistency \\
			%		$\Bels{i}\varphi \iimplies \Bels{i}\Bels{i}\varphi$ & Positive Belief Introspection \\
			%		$\tlnot\Bels{i}\varphi \iimplies \Bels{i}\tlnot\Bels{i}\varphi$ & Negative Belief Introspection\\
			$\Kns{i}\varphi \iimplies \Bels{i}\varphi$ & Knowledge implies Belief \\
			$\Bels{i}\varphi \iimplies \Poss{i}\Kns{i}\varphi$ & Weak Evidential Restraint\\
			%			$\Bels{i}\varphi \iimplies \Kns{i}\Bels{i}\varphi$ & Beliefs are Known\\
			From $\vdash \varphi$ and $\vdash \varphi \iimplies \psi$, infer $\vdash\psi$ & Modus Ponens\\
			From $\vdash \varphi$, infer $\vdash \Kns{i}\varphi$ & Necessitation of $\Kns{i}$\\
			\hline
		\end{tabular}
		\caption{Logic of Grounded Coherent Epistemic Agents}~\label{GC_agent}
	\end{center}
\end{table}

We call it a logic of Grounded Coherent Epistemic Agents, because their beliefs are grounded by objectively available evidence, and their beliefs are coherent. Neither the belief operator nor the knowledge operator is susceptible to L\"ob's Obstacle, as L\"ob's Theorem is not derivable in the system.

In the counterexample below, we see that the belief operator lacks positive introspection.

\begin{figure}[H]
	\begin{center}
		\begin{tikzpicture}[->,>=stealth',shorten >=1pt,auto,node distance=3cm,
		thick,base node/.style={circle,draw,minimum size=35pt}]
		
		\node[base node] (w) {\begin{tabular}{c}
			$w:  p$ \\ $\Bels{i}p$ \\ $\tlnot\Bels{i}\Bels{i}p$
			\end{tabular}};
		\node[base node] (v) [right of=w] {\begin{tabular}{c}
			$v:  p$ \\ $\tlnot\Bels{i}p $
			\end{tabular}};
		\node[base node] (u) [right of=v] {$u: \tlnot p$};
		\path[]
		(w) edge node[above] {$\Rel{b}^i$} (v)
		(v) 
		%		edge node[below] {} (w)
		%	\draw [->] (v) edge[in=5,out=355,loop] node[right] {$R$} (v)
		edge node[above] {$\Rel{b}^i$} (u)
		(u) edge [<-, loop above] node {$\Rel{b}^i$} (u);
		
		\end{tikzpicture}
	\end{center}
	\caption{A counterexample to $\Bels{i}\varphi\iimplies\Bels{i}\Bels{i}\varphi$.}
\end{figure}

Without positive belief introspection, L\"ob's Theorem for belief is no longer derivable, and therefore this (very weak) epistemic logic avoids L\"ob's Obstacle. The resulting logic is still in the Sahlqvist class, so it is sound and complete.

\begin{tcolorbox}
	\begin{lstlisting}[language=Coq]
	Theorem weak_evidential_restraint_is_sahlqvist: 
		forall (phi : prop) (a : DASL.Agents),
			sahlqvist_formula 
			(SB a (SProp phi) =s=> \ (SK a (\ (SK a (SProp phi))))).
	Proof.
		intros; sahlqvist_reduce. 
	Qed.
	\end{lstlisting}	
	
\end{tcolorbox}

We provide the following counterexample to L\"ob's Theorem.

\begin{figure}[H]
	\begin{center}
		\begin{tikzpicture}[->,>=stealth',shorten >=1pt,auto,node distance=4.5 cm,
		thick,base node/.style={circle,draw,minimum size=35pt}]
		
		\node[base node] (w) {\begin{tabular}{c}
			$w:  \Bels{i}(\Bels{i}p \iimplies p)$ \\ $\tlnot\Bels{i}p$
			\end{tabular}};
		\node[base node] (v) [right of=w] {\begin{tabular}{c}
			$v:  \Bels{i}p \iimplies p $ \\ $\tlnot p $
			\end{tabular}};
%		\node[base node] (u) [right of=v] {$u: \tlnot p$};
		\path[]
		(w) edge node[above] {$\Rel{b}^i$} (v)
		(v) 
		%		edge node[below] {} (w)
		%	\draw [->] (v) edge[in=5,out=355,loop] node[right] {$R$} (v)
		edge [<-, loop above] node {$\Rel{b}^i$} (v);
%		(u) edge [<-, loop above] node {$\Rel{b}^i$} (u);
		
		\end{tikzpicture}
	\end{center}
	\caption{A counterexample to $\Bels{i}(\Bels{i}\varphi \iimplies \varphi) \iimplies \Bels{i}\varphi$.}
\end{figure}

We assume in world $w$ that $\Bels{i}(\Bels{i}p \iimplies p)$ holds. From this it follows that in all doxastically accessible worlds, \emph{e.g.} $v$, $\Bels{i}p \iimplies p$ holds. This holds for $v$ when it has reflexive access only to itself, and $\tlnot p$ is the case. Because $\tlnot p$ is the case $v$, $\tlnot \Bels{i}p$ is the case at $w$, concluding the counterexample.

The logic also includes a sort of weakened positive introspection theorem about knowledge, from the Knowledge implies Belief axiom and Weak Evidential Restraint. It is $\Kns{i}\varphi \iimplies \Poss{i}\Kns{i}\varphi$, which we read as ``$i$ knows that $\varphi$ only if she has objectively available evidence that she knows $\varphi$". This seems intuitive, and does not allow L\"ob's theorem to destroy the integrity of knowledge. It is perhaps a satisfying compromise for those who find positive introspection about knowledge to be intuitive. The contrapositive of this weak positive introspection formula, $\Kns{i}\tlnot\Kns{i}\varphi \iimplies \tlnot\Kns{i}\varphi$ is an instance of the T axiom, so it turns out to have been a theorem all along anyway, for any epistemic logic with the Truth axiom for knowledge.

This logic allows us to make the key inference discussed in this thesis, when it serves as a static foundation for weakened \DASL.

\begin{theorem}[Weakened \DASL\ Inference of Safety-Critical Information]$\\$

$(\PalPos{A, \laa}true \tland \tlnot pre_s(\laa) )\iimplies \tlnot \Kns{i}pre_s(\laa)\tland \tlnot\Kns{i}\tlnot \Kns{i}pre_s(\laa)$.
\end{theorem}
\begin{proof}
	Assuming $w\models\PalPos{A,\laa}true$, it follows that $\Bels{i}pre_s(\laa)$, from minimum rationality. From weak evidential restraint, it follows that $\Poss{i}\Kns{i}pre_s(\laa)$, which is equivalent to $\tlnot \Kns{i}\tlnot\Kns{i}pre_s(\laa)$. From $\tlnot pre_s(\laa)$, it follows from the Truth axiom that $\tlnot \Kns{i}pre_s(\laa)$.
\end{proof}