\chapter{Introduction}
	\label{CH_Intro}

In this doctoral thesis, we present a logic for reasoning about the connection among four aspects of human agency: knowledge, belief, mere action, and safe action. A mere action is an action that has either been performed or not; its evaluation as a good or bad action, relative to some normative domain, is not considered. A safe action, on the other hand, is a mere action that is evaluated in a safety domain. Other normative domains could be considered without much change to the logic being required, but in this dissertation I confine myself to the domain of aviation safety.

Knowledge and belief are aspects of human agency that have been well-studied by philosophers, logicians, and computer scientists. Epistemology is a core field of philosophy, studying what knowledge is, and logicians have formalized these concepts. Computer scientists have taken these formalized systems and applied them to computational domains ranging from artificial intelligence to information security. The intersection of these fields at the knowledge nexus provides a rich variety of topics to study.

Similarly, the study of action has a history of richness at the intersection of philosophy, logic, and computer science. The relationship between actions and knowledge in particular finds rich application in the fields of economics and game theory. The logics for reasoning about these things have been actively developed over the past several decades, with a species of dynamic modal logic coming to the forefront as particularly well-suited to the task. 

Dynamic logic itself originated in computer science as a way to formally study the executions of programs. Philosophers and logicians then borrowed it back and married it with epistemic logic in order to generate logics of knowledge and action. The exchange of ideas between fields often occurs through the logics they use to study their subjects.

This dissertation picks up the thread from philosophical logic, specifically the efforts to use dynamic epistemic logic to model behavior in the rarefied world of games and toy examples. There researchers have made progress formalizing simple scenarios where one agent announces to the other something, and this causes the other agent's knowledge to increase. We extend that logic here in two ways. First, we experiment with its application to real humans piloting aircrafts. The environment is more complex, and challenges the logic to become a more realistic representation of human knowledge. To address this, we weaken the axioms of knowledge, and include axioms of belief and the relationship between knowledge and belief. The second extension concerns the normative aspect of actions. Dynamic epistemic logics impose preconditions on actions that determine whether they can be performed. We extend this idea with another precondition governing whether the action can be safely performed. It is a simple idea that opens up a wide new world of expressive power for the logics, as the underlying normative precondition function can be almost any normative aspect one cares to consider. One could swap out our safety precondition function with an ethical precondition function, or a legal precondition function, and develop similar dynamic logics of ethical or legal agency. Those applications are beyond the scope of this dissertation, however.

In order to validate the above extensions, we rely on rigorous tools to check our math, so to speak. Many other researchers have made great strides in mechanizing the proof and model theories of modal logics in the interactive theorem proving tool Coq. We follow their lead and similarly extend their efforts to bear out the logic we develop in the target application domain. We modify a deep embedding of dynamic epistemic logic in Cog to create a deep embedding of our logic. We verify the theorems of the object language in the target domain in Coq, as well as the soundness and completeness proofs in the metalanguage in Coq. While this is not a novel approach to logic research, it represents a more rigorous and computational approach to the methods of logic research that help move the dissertation from the merely philosophical domain and into the computer science domain. We do not develop any new theorem proving tools or techniques in the thesis, but our use of the interactive theorem prover itself is a feature that distinguishes this dissertation as one of computer science.

We have chosen the target domain of aviation safety, and it has a nice property that makes it a good candidate domain for extending the logic. Rather than modelling a wide array of behaviors and information that apply to human behavior generally, we need only model the behavior and information that relates to flying an airplane. We need to model the instrument readings and the pilot's inputs to the flight controls. This significantly cuts down the expressive space we need to handle. However, it is still a real human in a real world situation, requiring the logic to relax certain assumptions about the reasoning power of the agents and the information they have about the state of the world. Instrument readings can be conflicting, and pilots can be unaware of this conflict. This is not typically a situation logicians have tried to model in the past. Additionally, aviation safety is already a domain familiar with the use of formal methods to increase assurance, so the approach is not so novel as to be completely foreign to the target domain's researchers. This gives us related work to compare and contrast the present efforts to.

In particular, various logics exist for modelling safety properties of avionics systems, and tools for verifying that systems satisfy those properties. Aviation historically has been one of the few domains where the cost, perceived or actual, of formal verification is justified by the risks associated with system failure. We have engaged with this research community to some extent through collaborative research, conference presentations, and journal articles, and have received very helpful feedback. In some ways the approach adopted in this dissertation is too heavy handed for the particular needs of formal methods researchers in aviation safety. Rushby makes this case particularly well in his response to Ahrenbach and Goodloe \cite{AhrenbachGoodloe}. Many of the benefits achieved by the approach pursued here can be had through more straight forward integrations of modal logic with existing formal verification frameworks. Rushby achieves analogous results with merely a modal operator for a pilot's belief sufficing, with the belief treated as an input variable to the verification system just like any other.

However, our purposes here differ slightly from those of researchers focused on the aviation domain. An aviation-focused researcher asks `what is the best way to use this formal method in order to make aviation safer?', while we ask, `What domains are suitable proving grounds for this new general formal method?' We seek a level of generality beyond aviation, and so in some cases are unwilling to sacrifice the heavy machinery for the benefit of having a simpler pilot monitoring logic. We seek to develop a general logic of realistic agency, and we validate it in the proving ground of aviation safety. Just as a geneticist is interested in \emph{Drosophila melanogaster} (fruit flies), or an artificial intelligence researcher is interested in chess, because these subject (or target) domains are areas to apply new techniques and test new approaches~\cite{Kohler,Hsu}\footnote{In his book about the building of \emph{Deep Blue}, Hsu decries being called an AI researcher, and prefers his work to be seen as Very Large Scale Integration (VLSI) research, but the point stands that chess to him was a target domain useful as a proving ground for his primary research interest.}. 

The new approach pursued here, as mentioned earlier, is to relax the assumptions about agent rationality that other philosophical logics make, while augmenting the dynamic modality of action to include considerations of safety. If all goes well, this yields a logic for reasoning about realistic agents performing safe or unsafe actions, and about their beliefs and knowledge about those actions.

%When an honest but imperfect agent, possibly a human, performs an action, she reveals information about her knowledge and beliefs. For example, when Alice buys stock in Company A, it reveals that she believes the price of Company A's stock will increase. When Bob sees that Alice has done this, he might reconsider his belief that the stock price will decrease. In this way, her action serves as information to Bob about Company A's stock. If, on the other hand, Alice purchases the stock in secret, and she observes that Bob does not purchase stock in Company A, she can infer that Bob's information about Company A differs from her own. If she knows something about Company A that Bob does not, and this knowledge is what motivates her purchase of the stock, then she can infer that Bob lacks this particular knowledge. 




%The thesis advances the domain of modal logic by developing a rich and expressive logic suitable for reasoning about real humans in real situations, which in turn provides a new tool for formal methods researchers interested in developing safe human-machine hybrid systems. Thus, the thesis is interdisciplinary, pulling from fields as diverse as philosophy, game theory, computer science, and safety engineering.  
%%
%My work intersects with logic, game theory, information assurance, and aviation safety. I address a problem confronting formal methods researchers in that the vast majority of the methods used to verify system safety and security fail to address the human component. In both aviation safety, and more generally in complex systems, system failure often involves human behavior playing a critical role. I solve this problem by providing a mathematically precise logic for reasoning about the relationship between agents' actions and the safety-critical information they are aware of.

The logic, which I call Dynamic Agent Safety Logic (\DASL), is based on the logical foundations of game theory, in which models of agency formally capture how knowledge, rationality, and action relate to each other. Game theory presents a model that, given a description of a scenario, allows one to deduce what actions are dictated by a given theory of rationality. The standard game-theoretic inference works as follows:
\begin{equation*}
\mathit{Knowledge\_of\_Situation} \tland \mathit{Rationality} \iimplies \mathit{Good\_Action}.
\end{equation*}
One can read this as, ``if an agent has knowledge of a situation (\emph{e.g.} a game), and the agent is rational, then the agent executes a good action. In game theory, the important terms are suitably formalized for mathematical treatment. Knowledge is assumed to be perfect, rationality is defined as the maximization of some measure of utility, and good actions are those that bring about the outcomes with the most possible utility payoffs. The definitions make the inference an analytic truth.

Empirically, however, humans frequently deviate from the prescribed behavior. Looking at the above formula, we can ask a question: what can we infer when an agent fails to execute the prescribed action, as when pilots provide unsafe control inputs to their aircrafts? We can answer this question by examining the contrapositive of the above game-theoretic inference:
\begin{equation*}
\tlnot \mathit{Good\_Action} \iimplies \tlnot (\mathit{Knowledge\_of\_Situation} \tland \mathit{Rationality}),
\end{equation*}
or equivalently,
\begin{equation*}
\tlnot \mathit{Good\_Action} \iimplies \tlnot \mathit{Knowledge\_of\_Situation} \tlor \tlnot \mathit{Rationality}.
\end{equation*}
 
With a bit more Boolean manipulation, we have the following:
\begin{equation*}
\tlnot \mathit{Good\_Action} \tland \mathit{Rationality} \iimplies \tlnot \mathit{Knowledge\_of\_Situation}.
\end{equation*}
This can be read, ``If an agent is rational but executes a bad action, then the agent lacked knowledge of the situation." Thus, embedded in the classical game-theoretic model of agency is a logical inference from bad action to missing knowledge. This makes intuitive sense upon reflection. If someone is rational, yet they commit an irrational (read: ``bad") action, then it must be the case that they didn't know some crucial information. With this insight in hand, I identify a logic in which the above inference is sound, with details about which particular pieces of information are missing from an agent's knowledge base when she executes a bad action. Again, it should not be surprising that such a logic exists, because classical game theory already posits a \emph{logical} relationship between knowledge of particular propositions and particular actions.

I have formally captured such inferences with \DASL, where a (mostly) rational agent executes a bad action, and from this we can infer which safety-critical information they are missing. By developing a logic that can model the information flow in these situations, we advance the project of formally reasoning about human agency. Credit for initiating this project belongs to many researchers over the years, especially philosophers in the analytic tradition concerned with analyzing the epistemology and metaphysics of agency in a rigorous fashion. I have been most influenced by the works of the Amsterdam school of modal logic, led by Professor Johan van Benthem, where the efforts center around rich combinations of modal logics in order to model human agency. In~\cite{VB_TowardPlay}, they develop a modal logic for reasoning about the knowledge and decision-making of agents in games, and in~\cite{VB_LDII}, van Benthem explores similar themes around information flow and interaction.
%This can be done at run-time, as demonstrated herein by an encoding of the problem space for use by the Z3 theorem prover, which computes the missing information. We do not propose that a future avionics monitor actually run the Z3 tool on the aircraft, but rather use it here as a proof of concept, with implementation details left for future work.

% relevant part of the inference, formalized as a set of clauses in first order logic. The actual impThe prototype satisfies an information assurance property not yet treated formally by the literature, but done so here with \DASL. The property formalizes the idea that safety-critical information should not fail to reach a human and inform her actions. This is more than the information assurance property of \emph{availability}, because it is available to her only in a passive sense. It must be actively and specifically \emph{delivered} by ensuring that non-critical information does not compete for the human's awareness at critical moments. Formally specifying this property is another contribution of this thesis. 

%I apply these formal ideas to the domain of aviation safety, specifically incidents involving pilot errors that contribute to fatal mishaps. This is both an important area of research, where advancements can help save lives, but it also satisfies some desirable properties as a domain. When a new modal logic is developed, it is usually first applied to simple, closed domains, like games, involving simple agents with relatively few choices compared to the rich variability one can find in real life. Similarly, the environmental factors modeled in these situations are usually quite closed. To extend these logics into a realistic domain involving complex human agents and environments is quite a leap. The cockpit is a nice step between the two, because the relevant environment is not simple, but not as rich as a more general environment. The environment that must be modeled consists of discrete instruments with a somewhat limited range of possible values. The agents are real humans, but concerned with a limited number of actions involving inputs that manipulate the airplane's flight. Another advantage of the human agents in aviation is that they are highly trained, and so meet a level of rationality in the problem space that is not quite the full blown rationality of game theoretic agents, but is certainly better than an average human navigating a random problem encountered in the real world. Thus, human pilots in the cockpit are a Goldilocks zone of human agency that is more realistic than the agents in most game examples, but not quite as complex as humans in the more general problem space of reality writ large.

 

The history of research in this area dates back to the 1950s and '60s in the development of temporal logic (or tense logic) by Arthur Prior~\cite{Prior}, a graph theoretic semantics by Saul Kripke~\cite{Kripke}, and logics for belief and knowledge due to work by Rescher~\cite{rescher}, von Wright~\cite{vonWright}, and Hintikka~\cite{Hintikka}. These logics formalize reasoning about what was true, what will be true at some point, what is always going to be true, what is believed to be true, and what is known to be true. Each of these modifiers is a truth modality, and hence they each constitute a modal logic. The Amsterdam school and others built on these methods, especially the work by Patrick Blackburn~\cite{modal}, which illustrates the general features of any modal logic as a tool for reasoning about systems that can be modeled as graphs from an \emph{internal} perspective, whereas first order logic reasons about such systems from an external perspective. A first order formula might say what is true of the object \emph{x}, from a global perspective, but modal logic allows us to formalize truth from \emph{x}'s perspective. If the first order formula is $\forall x, \exists y:\ R(x,y) \tland P(y) \iimplies P(x)$, the corresponding modal formula would simply be $\Diamond_{R} p \iimplies p$. They both say the same thing: For any node $x$ in the graph, if it can reach a node $y$ by relation $R$, and $y$ is a node where the proposition ``\emph{y} is \emph{P}'' holds, then ``\emph{x} is \emph{P}'' holds. The former explicitly quantifies over the nodes, and the latter does so implicitly through the semantics, to be described later.

While these developments occurred in what might be called the philosophical branch of modal logic research, researchers in economics explored the mathematical foundations of game theory. Aumann, in~\cite{Aumann}, showed the axioms of epistemic logic that must be assumed in order for classical game theoretic results to hold. The agents in classical games, otherwise known as \emph{homo economicus}, are ideally rational, with perfect knowledge of their situations. We will meet these axioms and modify them for our purposes later.

Advances in dynamic modal logic come from theoretical computer science, where computer programs are modeled as state transition diagrams. As a program executes, the computer transitions from state to state, where each state is a collection of values assigned to variables, and each transition is a simple action executed. As this formalization lends itself to graph theoretic representation, it lends itself to formalization in modal logic, per Blackburn's insight. There are two main approaches to applying modal logic to the analysis of programs. The first is a static approach, where the entirety of the program's execution tree is modeled at once. Transitions from each state are captured by temporal logic. A program might be formalized in temporal logic, and the following theorem might be proven of it: \emph{At the source of the execution graph, it will always be true that bad event B does not occur}~\cite{Pnueli}. The second approach is dynamic, where each simple transition action A or B gets a modal operator, which allows us to reason about what happens after every execution of subprocess A, or after some executions of subprocess B, etc~\cite{HarelKozenTiuryn}. 

One thing that is easy to do with dynamic modal logic but somewhat complicated in the static approach is to model actions and knowledge. An epistemic logic is modeled by a static Kripke structure, and in the dynamic case this structure changes as the agents act and learn different things. The static approach requires a grand two dimensional Kripke structure with one dimension capturing the epistemic relations at a moment, and one dimension capturing the temporal relation as actions move forward through time. For an example of this approach, see John Horty~\cite{Horty}. For the dynamic approach, see van Ditmarsch \emph{et al.}~\cite{DEL}. 

Van Benthem and the Amsterdam school identified these various threads dispersed around campuses and saw how they related to each other, and how they might be fruitfully combined for various ends. As philosophers, they were mostly concerned with using the rich tools from economics and computer science to analyze human agency robustly and accurately. Modal logics offer tremendous expressive power at often a lower cost than first- or second order logic, because modal logics take an internal view of the graphs they reason about and are defined by. This often means that a powerful, useful modal logic can be defined that is also sound, complete, and \emph{decidable}. So, by carefully defining the modal operators for, say, knowledge, preference, and action, a modal logic of game theory can be developed; not just the epistemic aspect of the agents, but the games themselves, which van Benthem calls a Theory of Play. 

Applications of the Theory of Play have thus far been limited to relatively simple, artificial examples, in the same way methods in genetics research are often developed on fruit flies. In this thesis, I continue this work by extending application of the methods to richer real-world cases of humans in cockpits, which for reasons mentioned earlier make good cases for early forays into the formal modeling of human behavior. Just as genetics methods mature and eventually apply to humans, so must modal logic methods mature and apply to real humans in the world. 

%Information assurance properties have thus far dealt with systems whose components are entirely machine. Thus, properties like \emph{availability} assume that by guaranteeing the broadcast or even unicast of information suffices to guarantee that the information is useful to the receiver. If the component receiving the information is another machine, this assumption typically holds. However, we can see how this assumption might be violated in cases where the resources of the receiving device are overwhelmed, as is the case in a denial of service (DOS) attack. Merely making critical information available to the receiving device does not guarantee that it can receive it and make use of it. We would typically say that in this case, the availability property failed in the receiving device, as it had insufficient resources for processing the critical information. What would be called a denial of service attack in a machine to machine system is called \emph{information overload} when the receiver is a human. If we are modeling the sender and receiver as part of a larger system, and the receiver happens to be a human, it does not make sense to try to increase the availability of the brain's information processing resources by adding to them.  Instead, it makes sense to throttle down the competing but less critical information in order to ensure that the critical information reaches the receiver (human brain). I call this property \emph{delivery}. 

Dynamic Agent Safety Logic allows us to reason about which critical information is not being delivered to the human's brain. Because we can deduce which safety-critical information is missing from her knowledge base, we can automatically act to correct this failure of delivery, and therefore build systems that have a high assurance that the information will be delivered. I apply this technique to aviation safety as a formal method, but in principle it could be applied to other domains of human agency that meet certain conditions. Some examples that strike me as plausible include doctors and nurses in emergency rooms, cybersecurity analysts monitoring network traffic alerts, power plant operators, and of course motor vehicle drivers. During crises, these environments can quickly become saturated with alarms, and humans quickly suffer from information overload. If actions can be properly related to instruments such that unsafe actions can be detected, then the agent's missing knowledge can be deduced and rectified.

In order to adequately model human agency, we must depart from the familiar and comfortable logic that serves as the static base for most dynamic approaches: \SFive. This departure represents an innovation, which some in the formal methods community disagree with, so we must devote some time to motivating the move. We make the case that departing from \SFive\ as a static base representing human knowledge and belief makes sense for philosophical and technical reasons.

%This is where information assurance comes in. Information assurance is the field of computer science studying the desirable properties of information systems relating to information flow. In particular, information assurance is concerned with properties like confidentiality, integrity, and availability, among others. If a system is designed to interact with a human, then one of its desired properties is that the safety-critical information successfully flows to the human and informs her actions. Sometimes humans become overwhelmed by information competing for their attention, especially during emergency situations. This phenomenon is called information overload. It leads to human behavior that is suboptimal and often dangerous~\cite{hwang}. The problem, in terms of information assurance properties, is that some safety-critical information is not reaching the human component because the human component's cognitive resources are unavailable, suffering from a sort of denial-of-service attack. I propose to formally characterize this situation in my research, and offer strategies for automated enforcement of safety-critical information reaching the human component.

 
%My research validates the approach by using \DASL to analyze aviation mishaps, illustrating its usefulness. I formalize three aviation mishaps in \DASL, and this shows how \DASL allows for the inference of particular safety-critical information from actions. Establishing this logical inference is one thing, computing the inference is another. My thesis proceeds by using the Z3 Theorem Prover to compute the safety-critical information. My proposal is to extend this research by constructing a monitor prototype based on the logic suitable for runtime diagnosis of information misflow, that is, when safety-critical information fails to reach the human agent and inform her actions. Because we can deduce which safety-critical information is missing from her knowledge base, we can automatically act to correct this misflow.This is where information assurance comes in. Information assurance is the field of computer science studying the desirable properties of information systems relating to information flow. In particular, information assurance is concerned with properties like confidentiality, integrity, and availability, among others. If a system is designed to interact with a human, then one of its desired properties is that the safety-critical information successfully flows to the human and informs her actions. Sometimes humans become overwhelmed by information competing for their attention, especially during emergency situations. This phenomenon is called information overload. It leads to human behavior that is suboptimal and often dangerous~\cite{hwang}. The problem, in terms of information assurance properties, is that some safety-critical information is not reaching the human component because the human component's cognitive resources are unavailable, suffering from a sort of denial-of-service attack. I propose to formally characterize this situation in my research, and offer strategies for automated enforcement of safety-critical information reaching the human component.

In what follows, we describe the relevant background material in Chapter~\ref{CH_02}, including the foundations of game theory and the logical models of agency informing our new developments. Chapter~\ref{CH_03} motivates departure from \SFive\ as a static base and presents \DASL, then proves that it is sound and complete using the Coq Proof Assistant. Chapter~\ref{CH_04} applies \DASL\ to three aviation mishaps, again formalized in Coq. Chapter~\ref{CH_05} considers a problem facing logics that allow for reasoning about self-referential sentences, due to G\"odel and L\"ob's discoveries. We present L\"ob's Obstacle that these logics face, and show how to relax \DASL\ in order to avoid it, when the agents being modeled must be capable of self-reflection. In \ref{CH_summary} we summarize and conclude.