\chapter{Introduction}
	\label{CH_Intro}

In this doctoral thesis, I present a logic for reasoning about safety-critical information flow among machines and humans. The thesis advances the domain of modal logic by developing a rich and expressive logic suitable for reasoning about real humans in real situations, which in turn provides a new tool for formal methods researchers interested in developing safe human-machine hybrid systems. Thus, the thesis is interdisciplinary, pulling from fields as diverse as philosophy, game theory, computer science, and safety engineering.  
%
%My work intersects with logic, game theory, information assurance, and aviation safety. I address a problem confronting formal methods researchers in that the vast majority of the methods used to verify system safety and security fail to address the human component. In both aviation safety, and more generally in complex systems, system failure often involves human behavior playing a critical role. I solve this problem by providing a mathematically precise logic for reasoning about the relationship between agents' actions and the safety-critical information they are aware of.

The logic, which I call Dynamic Agent Safety Logic (\DASL), is based on the logical foundations of game theory, in which models of agency formally capture how knowledge, rationality, and action relate to each other. Game theory presents a model that, given a description of a scenario, allows one to deduce what actions are dictated by a given theory of rationality. The standard game-theoretic inference works as follows:
\begin{equation*}
\mathit{Knowledge\_of\_Situation} \tland \mathit{Rationality} \iimplies \mathit{Good\_Action}.
\end{equation*}
One can read this as, ``if an agent has knowledge of a situation (\emph{e.g.} a game), and the agent is rational, then the agent executes a good action. In game theory, the important terms are suitably formalized for mathematical treatment. Knowledge is assumed to be perfect, rationality is defined as the maximization of some measure of utility, and good actions are those that bring about the outcomes with the most possible utility payoffs. The definitions make the inference an analytic truth.

Empirically, however, humans frequently deviate from the prescribed behavior. Looking at the above formula, we can ask a question: what can we infer when an agent fails to execute the prescribed action, as when pilots provide unsafe control inputs to their aircrafts? We can answer this question by examining the contrapositive of the above game-theoretic inference:
\begin{equation*}
\tlnot \mathit{Good\_Action} \iimplies \tlnot (\mathit{Knowledge\_of\_Situation} \tland \mathit{Rationality}),
\end{equation*}
or equivalently,
\begin{equation*}
\tlnot \mathit{Good\_Action} \iimplies \tlnot \mathit{Knowledge\_of\_Situation} \tlor \tlnot \mathit{Rationality}.
\end{equation*}
 
With a bit more Boolean manipulation, we have the following:
\begin{equation*}
\tlnot \mathit{Good\_Action} \tland \mathit{Rationality} \iimplies \tlnot \mathit{Knowledge\_of\_Situation}.
\end{equation*}
This can be read, ``If an agent is rational but executes a bad action, then the agent lacked knowledge of the situation." Thus, embedded in the classical game-theoretic model of agency is a logical inference from bad action to missing knowledge. This makes intuitive sense upon reflection. If someone is rational, yet they commit an irrational (read: ``bad") action, then it must be the case that they didn't know some crucial information. With this insight in hand, I identify a logic in which the above inference is sound, with details about which particular pieces of information are missing from an agent's knowledge base when she executes a bad action. Again, it should not be surprising that such a logic exists, because classical game theory already posits a \emph{logical} relationship between knowledge of particular propositions and particular actions.

I have formally captured such inferences with \DASL, where a rational agent executes a bad action, and from this we can infer which safety-critical information they are missing. This can be done at run-time, as demonstrated herein by a prototype that uses the Z3 theorem prover to compute relevant part of the inference, formalized as a set of clauses in first order logic. The prototype satisfies an information assurance property not yet treated formally by the literature, but done so here with \DASL. The property formalizes the idea that safety-critical information should not fail to reach a human and inform her actions. This is more than the information assurance property of \emph{availability}, because it is available to her only in a passive sense. It must be actively and specifically \emph{delivered} by ensuring that non-critical information does not compete for the human's awareness at critical moments. Formally specifying this property is another contribution of this thesis. 

I apply these formal ideas to the domain of aviation safety, specifically incidents involving pilot errors that contribute to fatal mishaps. This is both an important area of research, where advancements can help save lives, but it also satisfies some desirable properties as a domain. When a new modal logic is developed, it is usually first applied to simple, closed domains, like games, involving simple agents with relatively few choices compared to the rich variability one can find in real life. Similarly, the environmental factors modeled in these situations are usually quite closed. To extend these logics into a realistic domain involving complex human agents and environments is quite a leap. The cockpit is a nice step between the two, because the relevant environment is not simple, but not as rich as a more general environment. The environment that must be modeled consists of discrete instruments with a somewhat limited range of possible values. The agents are real humans, but concerned with a limited number of actions involving inputs that manipulate the airplane's flight. Another advantage of the human agents in aviation is that they are highly trained, and so meet a level of rationality in the problem space that is not quite the full blown rationality of game theoretic agents, but is certainly better than an average human navigating a random problem encountered in the real world. Thus, human pilots in the cockpit are a Goldilocks zone of human agency that is more realistic than the agents in most game examples, but not quite as complex as humans in the more general problem space of reality writ large.

By developing a logic that can model the information flow in these situations, we advance the project of formally reasoning about human agency. Credit for initiating this project belongs to many researchers over the years, especially philosophers in the analytic tradition concerned with analyzing the epistemology and metaphysics of agency in a rigorous fashion. I have been most influenced by the works of the Amsterdam school of modal logic, led by Professor Johan van Benthem, where the efforts center around rich combinations of modal logics in order to model human agency. In~\cite{VB_TowardPlay}, they develop a modal logic for reasoning about the knowledge and decision-making of agents in games, and in~\cite{VB_LDII}, van Benthem explores similar themes around information flow and interaction. 

The history of research in this area dates back to the 1950s and '60s in the development of temporal logic (or tense logic) by Arthur Prior~\cite{Prior}, a graph theoretic semantics by Saul Kripke~\cite{Kripke}, and logics for belief and knowledge due to work by Rescher~\cite{rescher}, von Wright~\cite{vonWright}, and Hintikka~\cite{Hintikka}. These logics formalize reasoning about what was true, what will be true at some point, what is always going to be true, what is believed to be true, and what is known to be true. Each of these modifiers is a truth modality, and hence they each constitute a modal logic. The Amsterdam school and others built on these methods, especially the work by Patrick Blackburn~\cite{modal}, which illustrates the general features of any modal logic as a tool for reasoning about systems that can be modeled as graphs from an \emph{internal} perspective, whereas first order logic reasons about such systems from an external perspective. A first order formula might say what is true of the object \emph{x}, from a global perspective, but modal logic allows us to formalize truth from \emph{x}'s perspective. If the first order formula is $\forall x, \exists y:\ R(x,y) \tland P(y) \iimplies P(x)$, the corresponding modal formula would simply be $\Diamond_{R} p \iimplies p$. They both say the same thing: For any node $x$ in the graph, if it can reach a node $y$ by relation $R$, and $y$ is a node where the proposition ``\emph{y} is \emph{P}'' holds, then ``\emph{x} is \emph{P}'' holds. The former explicitly quantifies over the nodes, and the latter does so implicitly through the semantics, to be described later.

While these developments occurred in what might be called the philosophical branch of modal logic research, researchers in economics explored the mathematical foundations of game theory. Aumann, in~\cite{Aumann}, showed the axioms of epistemic logic that must be assumed in order for classical game theoretic results to hold. The agents in classical games, otherwise known as \emph{homo economicus}, are ideally rational, with perfect knowledge of their situations. We will meet these axioms and modify them for our purposes later.

The final foundational school of modal logic comes from theoretical computer science, where computer programs are modeled as state transition diagrams. As a program executes, the computer transitions from state to state, where each state is a collection of values assigned to variables, and each transition is a simple action executed. As this formalization lends itself to graph theoretic representation, it lends itself to formalization in modal logic, per Blackburn's insight. There are two main approaches to applying modal logic to the analysis of programs. The first is a static approach, where the entirety of the program's execution tree is modeled at once. Transitions from each state are captured by temporal logic. A program might be formalized in temporal logic, and the following theorem might be proven of it: \emph{At the source of the execution graph, it will always be true that bad event B does not occur.} The second approach is dynamic, where each simple transition action A or B gets a modal operator, which allows us to reason about what happens after every execution of subprocess A, or after some executions of subprocess B, etc. A state of the system is modeled, and it is updated based on the effects of the action modalities, making its representation in memory more efficient. The two approaches are extensionally equivalent, but they differ in flavor and ease of expression. 

One thing that is easy to do with dynamic modal logic but somewhat complicated in the static approach is to model actions and knowledge. An epistemic logic is modeled by a static Kripke structure, and in the dynamic case this structure changes as the agents act and learn different things. The static approach requires a grand two dimensional Kripke structure with one dimension capturing the epistemic relations at a moment, and one dimension capturing the temporal relation as actions move forward through time. For an example of this approach, see John Horty's~\cite{Horty}. For the dynamic approach, see van Ditmarsch \emph{et al.}~\cite{DEL}.

Van Benthem and the Amsterdam school identified these various threads dispersed around campuses and saw how they related to each other, and how they might be fruitfully combined for various ends. As philosophers, they were mostly concerned with using the rich tools from economics and computer science to analyze human agency robustly and accurately. Modal logics offer tremendous expressive power at often a lower cost than first- or second order logic, because modal logics take an internal view of the graphs they reason about and are defined by. This often means that a powerful, useful modal logic can be defined that is also sound, complete, and \emph{decidable}. So, by carefully defining the modal operators for, say, knowledge, preference, and action, a modal logic of game theory can be developed; not just the epistemic aspect of the agents, but the games themselves, which van Benthem calls a Theory of Play. 

Applications of the Theory of Play have thus far been limited to relatively simple, artificial examples, in the same way methods in genetics research are often developed on fruit flies. In this thesis, I continue this work by extending application of the methods to richer real-world cases of humans in cockpits, which for reasons mentioned earlier make good cases for early forays into the formal modeling of human behavior. Just as genetics methods mature and eventually apply to humans, so must modal logic methods mature and apply to real humans in the world. It turns out, as this thesis demonstrates, that using modal logic to analyze systems with real human components yields new information assurance insights. The information assurance property that falls out of the formal analysis of humans in cockpits is interesting, but should not be surprising.

Information assurance properties have thus far dealt with systems whose components are entirely machine. Thus, properties like \emph{availability} assume that by guaranteeing the broadcast or even unicast of information suffices to guarantee that the information is useful to the receiver. If the component receiving the information is another machine, this assumption typically holds. However, we can see how this assumption might be violated in cases where the resources of the receiving device are overwhelmed, as is the case in a denial of service (DOS) attack. Merely making critical information available to the receiving device does not guarantee that it can receive it and make use of it. We would typically say that in this case, the availability property failed in the receiving device, as it had insufficient resources for processing the critical information. What would be called a denial of service attack in a machine to machine system is called \emph{information overload} when the receiver is a human. If we are modeling the sender and receiver as part of a larger system, and the receiver happens to be a human, it does not make sense to try to increase the availability of the brain's information processing resources by adding to them.  Instead, it makes sense to throttle down the competing but less critical information in order to ensure that the critical information reaches the receiver (human brain). I call this property \emph{delivery}. 

Dynamic Agent Safety Logic allows us to reason about which critical information is not being delivered to the human's brain. Because we can deduce which safety-critical information is missing from her knowledge base, we can automatically act to correct this failure of delivery, and therefore build systems that have a high assurance that the delivery property is satisfied. I apply this technique to aviation safety as a formal method, but in principle it could be applied to other domains of human agency that meet certain conditions. Some examples that strike me as plausible include doctors and nurses in emergency rooms, cybersecurity analysts monitoring network traffic alerts, power plant operators, and of course motor vehicle drivers. During crises, these environments can quickly become saturated with alarms, and humans quickly suffer from information overload. If actions can be properly related to instruments such that unsafe actions can be detected, then the agent's missing knowledge can be deduced and rectified.

%This is where information assurance comes in. Information assurance is the field of computer science studying the desirable properties of information systems relating to information flow. In particular, information assurance is concerned with properties like confidentiality, integrity, and availability, among others. If a system is designed to interact with a human, then one of its desired properties is that the safety-critical information successfully flows to the human and informs her actions. Sometimes humans become overwhelmed by information competing for their attention, especially during emergency situations. This phenomenon is called information overload. It leads to human behavior that is suboptimal and often dangerous~\cite{hwang}. The problem, in terms of information assurance properties, is that some safety-critical information is not reaching the human component because the human component's cognitive resources are unavailable, suffering from a sort of denial-of-service attack. I propose to formally characterize this situation in my research, and offer strategies for automated enforcement of safety-critical information reaching the human component.

 
%My research validates the approach by using \DASL to analyze aviation mishaps, illustrating its usefulness. I formalize three aviation mishaps in \DASL, and this shows how \DASL allows for the inference of particular safety-critical information from actions. Establishing this logical inference is one thing, computing the inference is another. My thesis proceeds by using the Z3 Theorem Prover to compute the safety-critical information. My proposal is to extend this research by constructing a monitor prototype based on the logic suitable for runtime diagnosis of information misflow, that is, when safety-critical information fails to reach the human agent and inform her actions. Because we can deduce which safety-critical information is missing from her knowledge base, we can automatically act to correct this misflow.This is where information assurance comes in. Information assurance is the field of computer science studying the desirable properties of information systems relating to information flow. In particular, information assurance is concerned with properties like confidentiality, integrity, and availability, among others. If a system is designed to interact with a human, then one of its desired properties is that the safety-critical information successfully flows to the human and informs her actions. Sometimes humans become overwhelmed by information competing for their attention, especially during emergency situations. This phenomenon is called information overload. It leads to human behavior that is suboptimal and often dangerous~\cite{hwang}. The problem, in terms of information assurance properties, is that some safety-critical information is not reaching the human component because the human component's cognitive resources are unavailable, suffering from a sort of denial-of-service attack. I propose to formally characterize this situation in my research, and offer strategies for automated enforcement of safety-critical information reaching the human component.

In what follows, I will describe the relevant background material in Section~\ref{CH_02}, including the foundations of game theory and the logical models of agency informing my developments. In Section~\ref{CH_03}, I present the logic \DASL, and prove that it is sound and complete. In Section~\ref{CH_04}, I illustrate its application to three aviation mishaps, formalized in the Coq Proof Assistant. In Section~\ref{CH_05}, I formally specify the property of \emph{delivery}, and present a run-time monitor applied to the previously formalized case studies.