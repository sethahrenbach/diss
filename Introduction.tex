\chapter{Introduction}
	\label{CH_Intro}

In this doctoral thesis, I present a logic for reasoning about safety-critical information flow among machines and humans. The thesis advances the domain of modal logic by developing a rich and expressive logic suitable for reasoning about real humans in real situations, which in turn provides a new tool for formal methods researchers interested in developing safe human-machine hybrid systems. Thus, the thesis is interdisciplinary, pulling from fields as diverse as philosophy, game theory, computer science, and safety engineering.  
%
%My work intersects with logic, game theory, information assurance, and aviation safety. I address a problem confronting formal methods researchers in that the vast majority of the methods used to verify system safety and security fail to address the human component. In both aviation safety, and more generally in complex systems, system failure often involves human behavior playing a critical role. I solve this problem by providing a mathematically precise logic for reasoning about the relationship between agents' actions and the safety-critical information they are aware of.

The logic, which I call Dynamic Agent Safety Logic (DASL), is based on the logical foundations of game theory, in which models of agency formally capture how knowledge, rationality, and action relate. Game theory presents a model that, given a description of a scenario, allows one to deduce what actions are dictated by a given theory of rationality. The standard game-theoretic inference works as follows:
\begin{equation*}
\mathit{Knowledge\_of\_Situation} \tland \mathit{Rationality} \iimplies \mathit{Good\_Action}.
\end{equation*}
One can read this as, ``if an agent has knowledge of a situation (\emph{e.g.} a game), and the agent is rational, then the agent executes a good action. In game theory, the important terms are suitably formalized for mathematical treatment. Knowledge is assumed to be perfect, rationality is defined as the maximization of some measure of utility, and good actions are those that bring about the outcomes with the most possible utility payoffs. The definitions make the inference an analytic truth.

Empirically, however, humans frequently deviate from the prescribed behavior. Looking at the above formula, we can ask a question: what can we infer when an agent fails to execute the prescribed action, as when pilots provide unsafe control inputs to their aircrafts? We can answer this question by examining the contrapositive of the above game-theoretic inference:
\begin{equation*}
\tlnot \mathit{Good\_Action} \iimplies \tlnot (\mathit{Knowledge\_of\_Situation} \tland \mathit{Rationality}),
\end{equation*}
or equivalently,
\begin{equation*}
\tlnot \mathit{Good\_Action} \iimplies \tlnot \mathit{Knowledge\_of\_Situation} \tlor \tlnot \mathit{Rationality}.
\end{equation*}
 
With a bit more Boolean manipulation, we have the following:
\begin{equation*}
\tlnot \mathit{Good\_Action} \tland \mathit{Rationality} \iimplies \tlnot \mathit{Knowledge\_of\_Situation}.
\end{equation*}
This can be read, ``If an agent is rational but executes a bad action, then the agent lacked knowledge of the situation." Thus, embedded in the classical game-theoretic model of agency is a logical inference from bad action to missing knowledge. This makes intuitive sense upon reflection. If someone is rational, yet they commit an irrational (read: ``bad") action, then it must be the case that they didn't know some crucial information. With this insight in hand, I identify a logic in which the above inference is sound, with details about which particular pieces of information are missing from an agent's knowledge base when she executes a bad action. Again, it should not be surprising that such a logic exists, because classical game theory already posits a \emph{logical} relationship between knowledge of particular propositions and particular actions.

I have formally captured such inferences with DASL, where a rational agent executes a bad action, and from this we can infer which safety-critical information they are missing. This can be done at run-time, as demonstrated herein by a prototype that uses the Z3 theorem prover to compute relevant part of the inference, formalized as a set of clauses in first order logic. The prototype satisfies an information assurance property not yet treated formally by the literature, but done so here with DASL. The property formalizes the idea that safety-critical information should not fail to reach a human and inform her actions. This is more than the information assurance property of \emph{availability}, because it is available to her only in a passive sense. It must be actively and specifically \emph{delivered} by ensuring that non-critical information does not compete for the human's awareness at critical moments. Formally specifying this property is another contribution of this thesis. 

%Because we can deduce which safety-critical information is missing from her knowledge base, we can automatically act to correct this misflow.This is where information assurance comes in. Information assurance is the field of computer science studying the desirable properties of information systems relating to information flow. In particular, information assurance is concerned with properties like confidentiality, integrity, and availability, among others. If a system is designed to interact with a human, then one of its desired properties is that the safety-critical information successfully flows to the human and informs her actions. Sometimes humans become overwhelmed by information competing for their attention, especially during emergency situations. This phenomenon is called information overload. It leads to human behavior that is suboptimal and often dangerous~\cite{hwang}. The problem, in terms of information assurance properties, is that some safety-critical information is not reaching the human component because the human component's cognitive resources are unavailable, suffering from a sort of denial-of-service attack. I propose to formally characterize this situation in my research, and offer strategies for automated enforcement of safety-critical information reaching the human component.
%I apply this technique to aviation safety as a formal method, but in principle I believe it could be applied to many domains of human agency that meet certain conditions. 
%My research validates the approach by using DASL to analyze aviation mishaps, illustrating its usefulness. I formalize three aviation mishaps in DASL, and this shows how DASL allows for the inference of particular safety-critical information from actions. Establishing this logical inference is one thing, computing the inference is another. My thesis proceeds by using the Z3 Theorem Prover to compute the safety-critical information. My proposal is to extend this research by constructing a monitor prototype based on the logic suitable for runtime diagnosis of information misflow, that is, when safety-critical information fails to reach the human agent and inform her actions. Because we can deduce which safety-critical information is missing from her knowledge base, we can automatically act to correct this misflow.This is where information assurance comes in. Information assurance is the field of computer science studying the desirable properties of information systems relating to information flow. In particular, information assurance is concerned with properties like confidentiality, integrity, and availability, among others. If a system is designed to interact with a human, then one of its desired properties is that the safety-critical information successfully flows to the human and informs her actions. Sometimes humans become overwhelmed by information competing for their attention, especially during emergency situations. This phenomenon is called information overload. It leads to human behavior that is suboptimal and often dangerous~\cite{hwang}. The problem, in terms of information assurance properties, is that some safety-critical information is not reaching the human component because the human component's cognitive resources are unavailable, suffering from a sort of denial-of-service attack. I propose to formally characterize this situation in my research, and offer strategies for automated enforcement of safety-critical information reaching the human component.

In what follows, I will describe the relevant background material in Section~\ref{CH_02}, including the foundations of game theory and the logical models of agency informing my developments. In Section~\ref{CH_03}, I present the logic DASL, and prove that it is sound and complete. In Section~\ref{CH_04}, I illustrate its application to three aviation mishaps, formalized in the Coq Proof Assistant. In Section~\ref{CH_05}, I formally specify the property of \emph{delivery}, and present a run-time monitor applied to the previously formalized case studies.