\chapter{Background}
	\label{CH_02}

This chapter describes the context in which this dissertation makes advances. Section~\ref{sec:gametheory} lays out the basics of game theory, which provides a model of agency that we target and make more realistic. Section~\ref{sec:logic_foundation} introduces Dynamic Epistemic Logic, upon which the logic presented in this thesis is based, with axioms corresponding to the more realistic model of agency we develop. Section~\ref{sec:fm} describes the formal tools we use to mechanically check that \DASL\ is sound and complete, that its application to aviation safety is without errors, and to encode the inference of safety-critical information. Coq is both a programming language and a tool for verifying proofs, based on the Calculus of Inductive Constructions. We explain the basics of each of these to the reader in this section, since Coq is the tool we use for proof and application checking. Z3 is a theorem prover and model checker, which interract with solely for its SMT solving abilities. We encode the actual instrument readings as a first order formula, and we encode the safety precondition of an action as formulas comprising the constraining theory. We then check whether the formula is satisfiable modulo that theory. This tells us whether the intrument readings are consistent with the safety preconditions for an action, and if not, where the conflict lies. There is some additional complexity here, but we describe this in more detail in Chapter \ref{CH_05}.

%Section 2.3 describes related work involving logical approaches to agents in system verification. Section 2.4 describes the Coq Proof Assistant, a tool for constructing mechanically checked mathematical proofs, to be used in this research.



\section{Game Theory}~\label{sec:gametheory}

This section presents the basics of game theory in order to motivate the model of agency that \DASL\ seeks to formally capture. However, this dissertation does not seek to make advances in game theory itself. Future research could take the resulting logic and use it as a new foundation for game theoretic reasoning among more realistic agents, but that task is not taken on in the present work. We begin this section with a brief overview and an example game, and then present the model of agency required for the standard solution to the game.

Game theory is a mathematical model for strategic reasoning. Strategic reasoning refers to the way an agent reasons in situations where her payoffs depend on the actions of other agents in addition to her own, and in which she knows about these dependencies. For turn-based games, the mathematical structure employed is a \emph{game tree}, where each node represents a player's turn, and each edge the transition via a player's action. The leaves of the tree represent the payoffs each player receives at the end of the game. This paper is not concerned with the games themselves, but rather with the underlying assumptions about agency that entail their solutions. We briefly illustrate these underlying assumptions with the following example.

%Prisoner's Dilemma Matrix
%\begin{table}[!ht]
%	\centering	
%  	\setlength{\extrarowheight}{2pt}
%  	\begin{tabular}{*{4}{c|}}
%  		\multicolumn{2}{c}{} & \multicolumn{2}{c}{Player $B$}\\\cline{3-4}
%  		\multicolumn{1}{c}{} &  & $Cooperate$  & $Defect$ \\\cline{2-4}
%  		\multirow{2}*{Player $A$}  & $Cooperate$ & $(2,2)$ & $(10,0)$ \\\cline{2-4}
%  		& $Defect$ & $(0,10)$ & $(6,6)$ \\\cline{2-4}
%  	\end{tabular}
%\end{table}
\begin{figure}[ht]~\label{tree_ex}
	\begin{center}
		\small
		\begin{tikzpicture}[thin,
		level 1/.style={sibling distance=40mm},
		level 2/.style={sibling distance=30mm},
		level 3/.style={sibling distance=15mm},
		every circle node/.style={minimum size=1.5mm,inner sep=0mm}]
		
		\node[circle,draw,label=above:$A$] (root) {}
		child { node [circle,fill,label=above:$B$] (node-F) {}
			child { 
				node {$3,2$}
				edge from parent
				node[left] {$Left$}}
			child { 
				node (node-G) {$6,4$}
				edge from parent
				node[right] {$Right$}}
			edge from parent
			node[left] {$Left$}}
		child { node [circle,fill,label=above:$B$](node-E) {}
			child { 
				node[circle,fill,label=above:$A$](node-B) {}
				child {
					node {$3,0$}
					edge from parent
					node[left] {$Left$}}
				child {
					node (node-A) {$8,5$} 
					edge from parent
					node[right] {$Right$}}
				edge from parent
				node[left] {$Left$}}
			child { 
				node[circle,fill,label=above:$A$](node-C)  {}
				child {
					node (node-D) {$4,6$}
					edge from parent
					node[left] {$Left$}}
				child {
					node {$2,1$}
					edge from parent
					node[right] {$Right$}}
				edge from parent
				node[right] {$Right$}}
			edge from parent
			node[right] {$Right$}};
		\draw [very thick] (node-A) -- (node-B) 
		node[midway,above] {};
		\draw [very thick] (node-C) -- (node-D) 
		node[midway,above] {};
		\draw [very thick] (node-E) -- (node-C) 
		node[midway,above] {};
		\draw [very thick] (node-F) -- (node-G) 
		node[midway,above] {};
		\draw [very thick] (root) -- (node-F) 
		node[midway,above] {};
		\end{tikzpicture}
	\end{center}
	\caption{A game between players A and B}
\end{figure}

We can see in the figure below that the first player to act is Player A, at the root node. Her choices are to move \emph{Left} or \emph{Right}. Player B faces similar choices at the resulting nodes, and the players alternate turns until the game ends and they receive their payoffs, listed \emph{(A,B)}. Briefly glancing through the outcomes, it looks like A should aim for the node with payoff \emph{(8,5)}, because 8 is the highest payoff available to A. However, the official solution to the game is for A to first go \emph{Left}, and then for B to go \emph{Right}, resulting in a payoff of \emph{(6,4)}, where both get less than the intuitively appealing outcome! Why is this so?

Game theory makes strong assumptions about agent knowledge and rationality. The solution to this game is reached through an algorithm called \emph{backward induction}~\cite{VB_TowardPlay}. The players reason by starting at each end node and looking to the immediate parent node, and asking what the deciding player will do at that node, assuming she will choose the path with the highest payoff. So, at the bottom right of the figure, player A is to act, and she can go \emph{Left} for a payoff of 4, or \emph{Right} for a payoff of 2. So, she will go \emph{Left}, illustrated by the bold line. The end nodes not selected are subsequently eliminated. This process is repeated at each end node. Then it is recursively applied up the tree. So along the right branch, player B decides between \emph{Left} for a payoff of 5, or \emph{Right} for a payoff of 6, because B knows that A is rational, and he knows how she will act at each end node. A, at the root, then must choose between \emph{Left} for a payoff of 6, or \emph{Right} for a payoff of 4, because she knows that B is rational, and knows that B knows that she is rational. The explanation begins to illustrate the assumptions game theory makes about each player's knowledge. In fact, this only scratches the surface.

%The story is that Players A and B have been arrested for committing a crime together, and the prosecuting attorney offers them each the following deal: You and your partner in crime can either Defect from or Cooperate with each other. If you Defect and she Cooperates, I will let you go free, and come down hard on her with 10 years. If you both Defect, you will each get 6 years. If you both Cooperate with each other, I can only convict you of a lesser crime, which carries a sentence of 2 years. Some tellings assert that the defendants cannot communicate, but this does not matter under the classical model of rationality. No matter what the other does, each agent is better off by Defecting. Thus, the rational outcome of the game is for each agent to defect, each receiving 6 years. The counterintuitive aspect here is that if each agent have Cooperated, then they collectively would have been better off with 2 years each. But, this outcome cannot occur, because if one Cooperates, the other should Defect: 0 years is better than 2 years.

Game theory, and classical economics in general, makes the following assumptions about agent knowledge, formalized in epistemic logic~\cite{Aumann}. 

$\mathbf{Agency\  Model\  in\  Classical\  Game\  Theory}$.\\
(1) $\Kns{i}(\varphi \iimplies \psi) \iimplies (\Kns{i}\varphi \iimplies \Kns{i}\psi)$\\
(2) $\Kns{i}\varphi \iimplies \varphi$\\
(3) $\Kns{i}\varphi \iimplies \Kns{i}\Kns{i}\varphi$\\
(4) $\tlnot \Kns{i}\varphi \iimplies \Kns{i}\tlnot\Kns{i}\varphi$\\
(5) $\mathbf{C}_G ((1) \tland (2) \tland (3) \tland (4) \tland (5))$.

This forms an idealized model of the knowledge component of classical game theory's agents. $\Kns{i}$ is a modal operator for knowledge, and $\Kns{i}\varphi$ reads, ``agent \emph{i} knows that $\varphi$." $\mathbf{C}_G$ is a modal operator for common knowledge, the fixpoint for ``everyone in group G knows that everyone knows that..." The agents are logically omniscient due to (1), knowledge implies truth with (2), agents have \emph{positive introspection} with (3), and \emph{negative introspection} with (4). Assumptions (1), (3), (4), and (5) are somewhat dubious. The model also fails to formally represent other aspects of agency, like action and evaluation of outcomes. The model we propose makes weaker, more realistic assumptions about knowledge, includes a modal operator for belief, and formally represents action and the evaluation of actions as either safe or unsafe.

Recent work at the intersection of game theory and logic focuses on the information flow that occurs during games. Van Ditmarsch identifies a class of games called \emph{knowledge games}, in which players have diverging information~\cite{ditmarsch}. This slightly relaxes the assumption of classical game theory that players have common knowledge about each other's perfect information. Similarly, it invites logicians to study the information conveyed by the fact that an action is executed. For example, if the action is that agent $1$ asks agent $2$ the question, ``$p$?", the information conveyed is that $1$ does not know whether $p$, believes that $2$ knows whether $p$, and after the action occurs, this information becomes publicly known. The logic modeling games of this kind is of particular interest to us, as we are concerned with identifying the knowledge and belief state of human pilots based on their actions. 

The proceeding sections introduce the various logical systems that form a foundation for the work of this thesis, starting with modal logic in its traditional philosophical interpretation, and expanding to epistemic and doxastic logic. Then, we introduce dynamic logic, and its expansion into Dynamic Epistemic Logic and Public Announcement Logic.

\section{Modal Logic}\label{sec:logic_foundation}
Aristotle noted a distinction between contingent truth and necessary truth, and some Medieval philosophers continued this line of inquiry. Necessary truths could not have been otherwise. The definitions of natural numbers and the addition function guarantee that in all possible worlds, \emph{2 + 2 = 4}. On a plane, the truths of Euclidean geometry are necessarily true. ``There is life on Saturn's moon Enceladus" is a possible truth. ``Enceladus has water on it" is a contingent truth. What about ``Water is H20"? Modal logic was formalized in the early 1900's by C. I. Lewis and has recently had a resurgence in multidisciplinary interest~\cite{VB_MLOM}. At its core, modal logic allows us to reason about necessary and possible truths through the use of modal operators. What follows is a brief illustration of modal logic's concepts and formalisms.

When philosophers talk about necessity they usually mean \emph{metaphysical} necessity. In addition to formalizing this notion for systematic reasoning, modal logic is used for clarifying what exactly this means~\cite{Williamson}. Simple and intuitive examples are those from mathematics and notions of identity. Suppose $p$ is the arithmetic expression ``2 + 2 = 4". Obviously, $p$ is true in the actual world. We can make the stronger claim that $p$ is necessarily true, but what does this mean? As informal shorthands, initial attempts to define necessary truths might appeal, as I did above, to the claim that they could not possibly have been false. But then what do we mean by ``possible"? The formal semantics for dealing with these questions comes from Arthur Prior and Saul Kripke, and we introduce that machinery in the next section~\cite{Prior,Kripke}. For now, we say a statement is necessarily true if and only if it is true in all worlds we consider possible. The modal operator for necessity makes the formal statement: $\Box p$. Consider the following inference, where $\iimplies$ means ``implies": $\Box p \iimplies p$. This reads, ``Necessarily $p$ implies $p$," or equivalently, ``If $p$ is necessarily true, then $p$ is true." If $p$ is necessarily true, is $p$ true? Intuitively, the answer is `yes', and indeed a modal logic of metaphysical necessity includes this axiom for all formulas $\varphi$: $\Box \varphi \iimplies \varphi$. 

What other inferences can we make, based on our intuitive notion of necessity and possibility? What about ``if $p$ is true, then $p$ is possibly true"? To formalize this, we need the modal operator for possibility: $\Diamond$. So, the modal formula would be $p \iimplies \Diamond p$. This seems true as well, and indeed it is a theorem for metaphysical modal logic: $\varphi \iimplies \Diamond \varphi$. 
%In fact, those two axioms are equivalent. The definition of $\Diamond \equiv \tlnot \Box \tlnot$. This states that something is possible if and only if it is not necessarily false. It can also be said that something is necessary if and only if it is not possibly false: $\Box \equiv \tlnot \Diamond \tlnot$. We can now prove that the two axioms are equivalent.

%\begin{proof}
%	We first prove $\Box \varphi \iimplies \varphi$ implies $\varphi \iimplies \Diamond \varphi$, for any arbitrary $\varphi$ so long as it is the same on each side of the $\iimplies$. We begin by assuming $\Box \varphi \iimplies \varphi$. By the definition of $\Box$, we can substitute the equivalent formula $\tlnot \Diamond \tlnot \varphi \iimplies \varphi$. By contraposition, it holds that $\tlnot \varphi \iimplies \Diamond \tlnot \varphi$. But $\varphi$ is any arbitrary formula, including $\tlnot \varphi$. So we can substitute arbitrary formulas and get $\varphi \iimplies \Diamond \varphi$. What matters is that the $\varphi$ on the left and right of the $\iimplies$ are the same, not that they are the same in each axiom.
	
%	Next we prove that $\varphi \iimplies \Diamond \varphi$ implies $\Box \varphi \iimplies \varphi$, for arbitrary $\varphi$'s in each axiom. Assume $\varphi \iimplies \Diamond \varphi$. We can substitute in the equivalent formula using the definition of $\Diamond$ and get $\varphi \iimplies \tlnot \Box \tlnot \varphi$. By contraposition, it follows that $\Box \tlnot \varphi \iimplies \tlnot \varphi$. We substitute in $\varphi$ for $\tlnot \varphi$ as the arbitrary formula, and have $\Box \varphi \iimplies \varphi$.
%\end{proof}

It is obvious then that $\Box \varphi \iimplies \Diamond \varphi$ is a theorem. This states that if something is necessarily true then it is possibly true. What about the other direction: $\Diamond \varphi \iimplies \Box \varphi$? It turns out this is not a theorem under the typical notions of necessity and possibility. But this raises a question about how we would present a counterexample that disproves it. To do this, we need a semantics for the logic. The semantics we use are called \emph{possible world semantics}, and they are usually attributed to Saul Kripke~\cite{Kripke}. One would be hard-pressed to find a species of modal logic, whether in economics, computer science, or philosophy, that does not use possible world semantics in some form or another. Sometimes they are referred to as \emph{Kripke semantics}.

In possible world semantics, a graph structure is created with worlds as nodes and accessibility relations among worlds as the edges in the graph~\cite{modal}. Propositional formulas are true or false at each world. These graph structures are typically called Kripke structures. We can define the following Kripke structure, $\mathcal{M} = \{W, R, V\}$, where $W$ is a finite set of worlds, $\{w, v\}$, $R$ is a binary accessibility relation defined on those worlds $\{(w,v), (v,w)\}$, meaning $w$ has access to $v$ and $v$ has access to $w$, and $V$ is a \emph{valuation} function, which maps propositions to sets of worlds at which they are true. For example, if $p$ is true at $w$, then $w \in V(p)$. In our model we only care about the proposition $p$, which now stands for some contingent proposition, like ``all swans are white". Formally, we say $w \not\in V(p)$ if not all swans are white in world $w$, denoted by $\tlnot p$, while $w \in V(p)$ if they are, denoted by $p$. The following figure illustrates $\mathcal{M}$:

\begin{figure}[H]
	\begin{center}
	\begin{tikzpicture}[->,>=stealth',shorten >=1pt,auto,node distance=3cm,
	thick,base node/.style={circle,draw,minimum size=35pt}]
	
	\node[base node] (w) {$w: \tlnot p$};
	\node[base node] (v) [right of=w] {$v: p$};
	\path[]
	(w) edge node[above] {$R$} (v)
	    edge [loop left] node {$R$} (w)
	(v) edge node[below] {} (w)
%	\draw [->] (v) edge[in=5,out=355,loop] node[right] {$R$} (v)
	    edge [<-, loop right] node {$R$} (v);
	\end{tikzpicture}
	\end{center}
	\caption{$\mathcal{M}$: A simple counterexample using possible world semantics.}
\end{figure}

According to possible world semantics, $\Box \varphi$ is true at a world $w$, written $w \models \Box \varphi$, if and only if for all worlds $v$ such that $R(w,v)$ ($v$ is related to $w$ by the $R$ relation), $v \models \varphi$. This says a formula is necessarily true at a world if and only if it is true at all worlds accessible by that world according to the underlying $R$ relation. Similarly, $w \models \Diamond \varphi$ if and only if there is some world $v$ such that $R(w,v)$ and $v \models \varphi$. In $\mathcal{M}$, $w$ is $R$-accessible to itself, so there is a world accessible to $w$ where $p$ is false, and thus $w \models \tlnot \Box p$. However, since $v$ is $R$-accessible to $w$, and $v \models p$, it is true that $w \models \Diamond p$. Thus, we have $w \models \Diamond p$ and $w \models \tlnot \Box p$, a negation of $\Diamond p \iimplies \Box p$, so it cannot be the case that $\Diamond \varphi \iimplies \Box \varphi$ is a theorem, for arbitrary formula $\varphi$.

There are many systems of modal logic. The way any is distinguishedfrom  another is based entirely on the definition of $R$. For the popular \SFive, which we will examine later, $R$ is a Euclidean and eflexive binary relation on worlds. Thus, that is how ``possibility" is formally defined, and by extension, ``necessarily". That is also what determines the axioms and theorems comprising the logic. We formally define these notions in the next section, along with the syntax and semantics of propositional modal logic.

\subsection{Modal Logic Syntax and Semantics}~\label{sec:molo_syn_sem}
This section formally defines the syntax, what the logic looks like, and the semantics, what the truth conditions are for the logic.

Recall that Boolean logic is a simple logic for reasoning about basic propositions using the logical connectives `and', `or', `not', and `if...then'. It forms the foundation of most logics and has applications ranging from philosophy to circuit design. Propositions are represented as constants {\emph p,\ \emph q} and well-formed formulas of the language are constants and any proper combination of constants using the above logical connectives, represented symbolically as,
$$\varphi \stackrel{def}{=} p\  |\  \varphi \wedge \varphi\  |\  \varphi \vee \varphi\  |\ \neg \varphi \ |\   \varphi \iimplies \varphi. 
$$

As illustrated in the previous section, modal logic adds to propositional logic with modal operators for necessary and possible truths. The syntax for propositional logic is extended in the following way to make modal logic:
$$\varphi \stackrel{def}{=} p\  |\  \varphi \wedge \varphi\  |\  \varphi \vee \varphi\  |\ \neg \varphi \ |\   \varphi \iimplies \varphi\ |\ \Box \varphi \ |\ \Diamond \varphi. 
$$

The semantics for Boolean logic are simply truth tables for each connective, which I will not reproduce here. However, the operators in modal logic are not truth functional, and require more complex semantics, which we earlier mentioned are called possible world semantics. They are as follows.

Let $\mathcal{M} = \{W, R, V\}$ be a model such that $W$ is a set of possible worlds, $R$ is a binary relation on those worlds, and $V$ is a valuation function mapping atomic propositions to the sets worlds satisfying them,  

\begin{align*}
w \models p\  &\mathbf{iff}\ w \in V(p) \\
w \models \neg \varphi\  &\mathbf{iff}\  w\not\models \varphi  \\
w \models \varphi \wedge \psi\ &\mathbf{iff}\ w\models \varphi\ \mathnormal{and}\ w\models \psi \\
w \models \Box \varphi\  &\mathbf{iff}\ \forall v,\ R(w,v)\  \mathnormal{implies}\ v \models \varphi. \\
w \models \Diamond \varphi\ &\mathbf{iff}\ \exists v,\ R(w,v)\ \mathnormal{and}\ v\models \varphi.
\end{align*}

The character of a modal logic is determined by the binary relation on worlds underlying the modal operators. We mentioned \SFive\ in the previous section as having a $R$ relation in which every world is accessible to itself by the binary relation. It is also one in which the $R$ relation is Euclidean. These conditions are called a \emph{frame} conditions, and the models that satisfy these conditions belong to said frame. All reflexive frames have the following frame conditions:
\begin{equation}
\forall x,\ R(x,x)
\end{equation}
Likewise, all reflexive frames have the following axiom:
\begin{equation}
	\Box \varphi \iimplies \varphi
\end{equation}
It is not just a stipulation that all reflexive frames must have that axiom: They have that axiom \emph{because} they have that frame condition. This is due to correspondence theory, which we explain later on.

Relaxing a frame condition changes the $R$ relation, and in doing so changes the logic. If we remove the reflexivity condition, the above axiom is no longer an axiom for the logic defined on $\mathcal{M}$. This means that we can specify the axioms we want by specifying the frame condition on the accessibility relation. Each frame condition corresponds to a modal logic axiom. In addition to reflexivity, the other common frame conditions are as follows, with their corresponding modal logic axiom:
\begin{itemize}
	\item Transitivity
	\begin{eqnarray}
	\forall x,y,z\ R(x,y) \tland R(y,z) \iimplies R(x,z)\\
	\Box \varphi \iimplies \Box \Box \varphi
	\end{eqnarray}
	\item Symmetry
	\begin{eqnarray}
	\forall x,y\ R(x,y) \iimplies R(y,x)\\
	\varphi \iimplies \Box \Diamond \varphi
	\end{eqnarray}
	\item Euclidean
	\begin{eqnarray}
	\forall x,y,z\ R(x,y) \tland R(y,z) \iimplies R(x,z)\\
	\Diamond \varphi \iimplies \Box \Diamond \varphi
	\end{eqnarray}
	\item Seriality
	\begin{eqnarray}
	\forall x \exists y, R(x,y) \\
	\Box \varphi \iimplies \Diamond \varphi
	\end{eqnarray}
\end{itemize}

These conditions are not exhaustive, but they represent some of the commonly combined conditions used to define axioms of different modal logics. By combining frame conditions, a modal operator is defined with the properties desired. For example, if we wish to define a modal operator for reasoning about the knowledge of ideally rational agents, as is done in Fagin \etal\cite{FHMV}, we impose the frame conditions of reflexivity and Euclidicity (Euclideannes), or one that is transitive and reflexive, as in Hintikka~\cite{Hintikka}. However, if we wish to develop a logic for belief, as the previously cited works also do, we must impose transitivity, Euclidicity, and seriality. The reasons for this are explored in the following sections.

\section{Epistemic Logic}~\label{epistemic_logic}
Epistemic logic began with philosophical concerns about knowledge ~\cite{FHMV,Hintikka,rescher}. Presently, it is likely to be studied in computer science as the logic for reasoning about ideally rational agents in well-structured environments. The agents are ideally rational in the sense that there is no bound on how much they can be said to know, nor on what propositions they are aware of at any one time. Thus, they have no problem conceiving of every possible sequence of moves a game may consist of, evaluating all possible outcomes, and deducing the optimal sequence of moves in order to maximize their own utilities. They are even stronger than contemporary computers in this way, which are bounded by time and space, and so are unable to actually compute the ideal strategy for an otherwise solvable game like Go. An ideally rational agent can solve Go and compute the game tree all the way to its end. These are the agents of game theory, which always select the optimal move when they have perfect information about the game and the other players.

The axioms that specify this level of knowledge are those introduced in Section~\ref{gametheory}, and recounted here without reference to common knowledge, which complicates things too much for our purposes:

$\mathbf{Agency\  Model\  in\  Classical\  Game\  Theory}$.\\
(1) $\Kns{i}(\varphi \iimplies \psi) \iimplies (\Kns{i}\varphi \iimplies \Kns{i}\psi)$\\
(2) $\Kns{i}\varphi \iimplies \varphi$\\
(3) $\Kns{i}\varphi \iimplies \Kns{i}\Kns{i}\varphi$\\
(4) $\tlnot \Kns{i}\varphi \iimplies \Kns{i}\tlnot\Kns{i}\varphi$.

These axioms make the knowledge operator an $\mathit{S5}$ modal operator. Strictly speaking, including (3) is unnecessary because it follows as a theorem from (2) and (4), but we include it so that the reader is not surprised when we refer to it.

The first axiom holds for all \emph{normal} modal logics, and under the epistemic interpretation, it states that if an agent knows that $\varphi$ implies $\psi$, and she knows $\varphi$, then she knows $\psi$. This is intuitive enough on the first pass, but can lead to a problem of global skepticism. If $i$ knows that she is an embodied agent in the external world only if she knows that she is not a brain in a vat perceiving a simulated reality, then she knows she is an embodied agent only if she knows she is not a brain in a vat. It seems clear that she cannot know that she is not a brain in a vat, because her sensory experience is compatible with either the embodied scenario of the brain in a vat scenario. Therefore, she does not know she is an embodied agent in the external world. Thus, many philosophers question whether knowledge for human-like agents is closed under logical implication like this.

The second axiom states that a known proposition must be true, and finds ample support from philosophers devoted to studying the nature of knowledge. It can be considered the property of knowledge that most distinguishes it from belief, because beliefs can be false.

The third axiom, \emph{positive introspection}, states that an agent knows something only if she knows \emph{that} she knows it. This imposes a high standard on knowledge, under the assumption that an agent can identify the conditions that guarantee the truth of a known proposition, and in being able to do so, are justified to a sufficient degree. However, this property is largely rejected by philosophers, as this requirement is thought to be largely unattainable. 

Finally, \emph{negative introspection} states that an agent does not know something only if she knows that she does not know it. This has an undesirable effect of creating knowledge out of ignorance, for if $i$ does not know that she does not know $\varphi$, this is sufficient for inferring that she knows $\varphi$, according to negative introspection. This is fine for ideally rational agents, and indeed it is a noble aspirational goal to know what one is ignorant about, but for human-like agents it is entirely unrealistic as an axiom.

These axioms are firmly rooted in the literature of formal epistemology and epistemic logic, and relaxing them in order to model more realistic agents requires a word or two. This section introduces and defends our relaxation of the classical axiomatization.

\subsection{Why S5 Is Appealing} 
Before presenting our relaxed axiom schema for knowledge, we acknowledge the appealing properties of the S5 knowledge operator. Logicians who adopt the S5 knowledge operator have good reasons for doing so, both technical and intuitive. The technical appeal lies in the fact that the S5 operator is well-behaved from a logical point of view. Epistemic logics with S5 operators are decidable because S5 operators have semantics that satisfy the finite model property, meaning that every invalid proposition has a finite counter-model. The S5 knowledge operator allows a formula prefixed by any arbitrary combination of $\Kns{i}$ and $\Poss{i}$ symbols to be reduced to a formula prefixed by at most two. This keeps the models small.

On the intuitive side, the relations that define the semantics of an S5 knowledge operator are equivalence relations. They are defined as reflexive and Euclidean, which implies that they are also transitive and symmetric. Equivalence relations capture a notion of indistinguishability because each world in the equivalence class is indistinguishable from each other. All the modal formulas true at one of the worlds in an equivalence relation are true at each of the other worlds in the relation, so from a modal perspective, they are indistinguishable. When the modality is knowledge, this means that relative to the agent's knowledge, the worlds are indistinguishable. 

These properties of \SFive\ epistemic logic are strong points in its favor. We suppose that any formal system seeking to model human-like knowledge must capture the intuitive idea that an agent does not know \emph{whether} a proposition $\varphi$ is true or false just in case she cannot distinguish the world she is in from the possible worlds in which either might be the case. Distinguishability seems to imply that there is some evidence possessed by the agent that allows her to rule out possibilities, and therefore gain knowledge.

The technical benefit of the finite model property makes \SFive\ a friendly logic to work with, but it is not a reason to believe that the theory of knowledge described by \SFive\ is true of human agents. Therefore, we do not see the need to impose this condition as a requirement for realistic epistemic logics. However, it would be desireable to achieve the finite model property while maintaining realism.

Next we turn to a brief summary of doxastic logic, which shall play a role in \DASL's static foundation. 

\section{Doxastic Logic}
This section presents doxastic logic, the logic for reasoning about belief. Hintikka's formalization in \cite{Hintikka} of the belief operator is standard, and we adopt it here. The operator $\Bels{i}$ for `agent $i$ believes $\ell$' is defined by a binary relation on worlds that is serial and Euclidean. It is a normal modal operator, and the property of transitivity follows from seriality and Euclidicity. So, the model of an agent's belief is defined by the following axioms,

$\mathbf{Hintikka\ Belief\ Model}$.\\
(1) $\Bels{i}(\varphi \iimplies \psi) \iimplies (\Bels{i}\varphi \iimplies \Bels{i}\psi)$\\
(2) $\Bels{i}\varphi \iimplies \BPoss{i}\varphi$\\
(3) $\Bels{i}\varphi \iimplies \Bels{i}\Bels{i}\varphi$\\
(4) $\tlnot \Bels{i}\varphi \iimplies \Bels{i}\tlnot\Bels{i}\varphi$.

Clearly the primary difference between this model of belief and the model of knowledge from classical game theory is axiom (2). Rather than guaranteeing truth, beliefs guarantee consistency. One could correctly argue that this is too strong of an assumption for humans, but we leave relaxations of the belief model to future work, and are content to relax the idealizations about knowledge in this dissertation. Similarly, reasonable cases could be made against axioms (3) and (4).

The logic associated with these axioms is called $\mathcal{KD}\mathit{45}$. On the doxastic interpretation, it represents a slight idealization of how humans actually adopt beliefs. Clearly, humans can have contradictory beliefs, and they may believe something without believing that they believe it. Cases of cognitive dissonance like that are idealized away from, not only because modeling such a psychology is complicated, but also because we seek a logic that maintains some level of normativity. If someone believes two contradictory propositions, she should be able to give one up if this contradiction is pointed out to her, and furthermore she is rational to do so. This reflects Hintikka's goal of developing logics of knowledge and belief that allow us to consistently identify when attributions of knowledge and belief are in some sense criticizable. Maintaining a notion of normativity in a logic is important to us, and underlies some the idealizations we accept in \DASL's static base logic.

%This is formalized in a couple different ways in the literature, sometimes as we did before mapping a proposition-world pair to a Boolean, sometimes defining propositions as sets of worlds, where $V(w)\in p$ if and only if $p$ is true at world $w$. The former convention will be followed here.



%\subsection{Avionics and Formal Methods}
%\section{Logical Framework}~\label{logic_foundation}
%This section introduces the logic used as a tool to formally model and reason about pilots in emergency situations. For most readers the logic to be used will be sufficiently complex and unfamiliar that a gradual approach is warranted, beginning with the basics of familiar Boolean logic, and advancing to the final logic specially developed for reasoning about pilot error.
%\subsection{Logical Foundations} 
%Recall that Boolean logic is a simple logic for reasoning about basic propositions using the logical connectives `and', `or', `not', and `if...then'. It forms the foundation of most logics and has applications ranging from philosophy to circuit design. Propositions are represented as constants {\emph p,\ \emph q} and well-formed formulas of the language are constants and any proper combination of constants using the above logical connectives, represented symbolically as,
%$$\varphi \stackrel{def}{=} p\  |\  \varphi \wedge \varphi\  |\  \varphi \vee \varphi\  |\ \neg \varphi \ |\   \varphi \rightarrow \varphi. 
%$$

%Boolean logic is limited in what it can express, however. Some situations require higher fidelity, for example when reasoning about knowledge. Consider the proposition, ``If Alice knows $p$, then $p$ is true." Philosophers of knowledge agree with this proposition, considering it a valid claim about knowledge, based on the principle that all known propositions are true. However, this validity cannot be faithfully represented in propositional logic as a general validity. Each sentence in the claim is assigned its own constant, resulting in a formalization like, ``$\mathit{a} \rightarrow \mathit{p}$," with {\emph a} representing ``Alice knows $p$," and $p$ representing ``$p$ is true." But $\mathit{a} \rightarrow \mathit{p}$ is not a validity of Boolean logic. 

%To represent reasoning about knowledge, and other similar domains requiring higher fidelity, epistemic operators are added to the basic Boolean logic, resulting in the following logic:

%$$\varphi \stackrel{def}{=} p\  |\ \mathbf{K_{i}}\varphi\ |\ \neg \varphi\ |\ \varphi \wedge \varphi, 
%$$
%where $\mathbf{K_{i}}\varphi$ states that agent ${\boldmath i}$ \emph{knows} that \emph{$\varphi$}, allowing us to formally represent the above validity:
%$$\mathbf{K_{Alice}}p \rightarrow p.$$

%Semantics for epistemic logic are given by Kripke structures, which serve as models by which epistemic formulas are evaluated. At its core, a Kripke structure is a graph with nodes and edges, accompanied by a function determining which atomic propositions are true at which worlds. The nodes are normally thought of as possible worlds, or as possible states of the system being modeled. The edges are normally thought of as possibility relations among worlds or states. If, at a node representing a world {\emph w}, agent {\emph A} considers it possible that she is in world {\emph v}, the Kripke semantics modeling this situation would have world {\emph w}'s node connected to world {\emph v}'s node by an edge representing {\emph A}'s epistemic possibility relation.

%Formally, we say a Kripke structure is a \emph{tuple} $\langle W,\ V,\ Agents,\ \{R_{i}|i\in Agents\}\rangle,$ where \emph{W} is a set of worlds, \emph{V} is a function from propositional constants to sets of worlds satisfying the proposition, \emph{Agents} is a set of agents, and each $R_{i}$ is agent \emph{i}'s epistemic possibility relation.

%The semantics are as follows, for worlds $w,\ v\ \in W$:
%\begin{align*}
%w \models p\  &\mathbf{iff}\ w \in V(p) \\
%w \models \neg \varphi\  &\mathbf{iff}\  w\not\models \varphi  \\
%w \models \varphi \wedge \psi\ &\mathbf{iff}\ w\models \varphi\ \mathnormal{and}\ w\models \psi \\
%w \models \mathbf{K_{i}}\varphi\  &\mathbf{iff}\ \forall v,\ wR_{i}v\  \mathnormal{implies}\ v \models \varphi.
%\end{align*}

%Early applications of epistemic logic in information security modeled system components as agents whose knowledge represented the information flowing to them. Recently, a modified version of epistemic logic known as dynamic epistemic logic has been used in information security to formally reason about security properties involving human components of systems. The research described in this proposal advances along similar lines, treating pilots as human components of safety critical aviation systems and using dynamic epistemic logic to reason about them. The next section describes the basic dynamic epistemic logic.

\subsection{Dynamic Epistemic Logic}~\label{delsubsection}
Dynamic Epistemic Logic (DEL) formalizes situations in which agents' epistemic states change over time, due to announcements or other informational events.~\cite{DEL} For example, if Alice truthfully and trustworthily communicates to Bob that $\varphi$, then after this informational even it is true that Bob knows $\varphi$. This situation cannot be modeled by the epistemic logic introduced in the previous section. To model it, we introduce the following formal machinery.

To capture informational events, we introduce the idea of relativizing a Kripke structure. In the previous example, if we model the Alice and Bob situation prior to Alice's communication, we can have a world $w$ from which Bob considers $\varphi$- as well as $\neg\varphi$-worlds possible. However, after the informational event, Bob knows $\varphi$, so the model is \emph{relativized} to a submodel in which only $\varphi$-worlds are accessible by Bob's epistemic possibility relation. Thus, after the informational event, the model transitions to a submodel with fewer edge relations.

The logic for reasoning about information flow in knowledge games is called Dynamic Epistemic Logic (DEL). As its name suggests, it combines elements of epistemic logic and dynamic logic. Epistemic logic is the static logic for reasoning about knowledge, and dynamic logic is used to reason about actions. In dynamic logic semantics, nodes are states of the system or the world, and relations on nodes are transitions via programs or actions from node to node. If we think of each node in dynamic logic as being a model of epistemic logic, then actions become relations on models, representing transitions from one multi-agent epistemic model to another. For example, if we have a static epistemic model $M1$ representing the knowledge states of agents Alice and Bob at a moment, then the action $``\varphi"$ is a relation between $M1$ and $M2$, a new static epistemic model of Alice's and Bob's knowledge after the question is asked. All of this is captured by DEL.

\begin {center}
\begin {tikzpicture}[-latex ,auto ,node distance =3 cm and 4cm ,on grid ,
semithick ,
state/.style ={ circle ,top color =white , bottom color = white ,
	draw, text=black , minimum width =1 cm}]

\node[state] (A)  {$M1$};
\node[state] (B) [right =of A] {$M2$};

\path (A) edge  node[above] {$``\varphi"$} (B);
%\path (C) edge [bend left =25] node[below =0.15 cm] {$1/2$} (A);
%\path (A) edge [bend right = -15] node[below =0.15 cm] {$1/2$} (C);
%\path (A) edge [bend left =25] node[above] {$1/4$} (B);
%\path (B) edge [bend left =15] node[below =0.15 cm] {$1/2$} (A);
%\path (C) edge [bend left =15] node[below =0.15 cm] {$1/2$} (B);
%\path (B) edge [bend right = -25] node[below =0.15 cm] {$1/2$} (C);
\end{tikzpicture}
\end{center}


The above figure illustrates the relationship between static epistemic models and dynamic logic models. As a purely dynamic model, the figure shows the action $``p?"$ transitioning between nodes $M1$ and $M2$. If we zoom in on the nodes, we see their structure as epistemic models, with their own nodes and edges, representing possible worlds and epistemic relations.
\begin{figure}[H]
\begin{center}
\begin{tikzpicture}[->,>=stealth',shorten >=1pt,auto,node distance=3cm,
thick,base node/.style={circle,draw,minimum size=35pt}]

\node[base node] (w) {$w: \tlnot \varphi$};
\node[base node] (v) [right of=w] {$v: \varphi$};
\path[]
(w) edge node[above] {$Bob$} (v)
edge [loop left] node {$Bob$} (w)
(v) edge node[below] {} (w)
%	\draw [->] (v) edge[in=5,out=355,loop] node[right] {$R$} (v)
edge [<-, loop right] node {$Bob$} (v);
\end{tikzpicture}
\end{center}
\caption{$M1$: The model before Alice announces $``\varphi"$.}
\end{figure}

\begin{figure}[H]
	\begin{center}
		\begin{tikzpicture}[->,>=stealth',shorten >=1pt,auto,node distance=3cm,
		thick,base node/.style={circle,draw,minimum size=35pt}]
		
		\node[base node] (v) [right of=w] {$v: \varphi$};

		\end{tikzpicture}
	\end{center}
	\caption{$M2$: The model after Alice announces $``\varphi"$.}
\end{figure}

We are concerned with an additional element: the \emph{safety} status of an action, and an agent's knowledge and belief about that. To capture this, we extend DEL and call the new logic Dynamic Agent Safety Logic (\DASL), which we introduce in the next chapter. The next section lays out the state of formal methods involving human-machine systems.

\section{Formal Methods Tools}~\label{sec:fm}
%The field of formal methods grew out of attempts to verify the correctness of hardware and software. It was normally confined to systems deemed mission- or safety-critical, due to its high cost. Arguments for the use of formal methods normally involve an anecdote in which a system fails due to some sort of error missed by traditional testing methods but that would have been caught and prevented were formal methods used.
%Formal methods increase confidence that hardware and software components function correctly~\cite{RushbyFMbook}. They involve the application of mathematical techniques to the design and analysis of systems, usually the safety-critical components~\cite{johnson_butler_fm}. A standard approach is to develop an abstract specification of the software or hardware system, design the system to meet those specifications, and then use formal logic to prove that the designed system meets the desired safety specifications. 

%According to the Federal Aviation Administration (FAA), one of the top 12 causes of mistakes in the aviation workplace is a lack of awareness~\cite{faa}. According to Boeing and the FAA, approximately 80 per cent of aviation accidents (including maintenance accidents) are due to human error~\cite{boeing,faaHF}. To increase safety in human factors, the industry relies on education, psychology, anthropometrics, safety engineering, and to a limited extent, computer science. The primary focus of computer science research regarding human factors is the design and testing of software systems that are easy and intuitive for humans to interact with. 

%Formal methods are traditionally deployed in one of two ways. The more painful but perhaps more commonly used approach is to perform \emph{semantic archeology}, wherein an artifact that has already been constructed is formalized into a mathematically rigorous language and then reasoned about. The way urged by most contemporary researchers involves continuously specifying and verifying correctness throughout the development process, prior to system deployment. To facilitate this preferred approach, most formal methods researchers devote themselves to developing tools that are easy to use.
%
%The intended use of these tools, however, has remained the same: to verify the correctness of hardware and software systems, or to provide assistance to mathematicians attempting to prove theorems. At bottom, the tools allow computers to provide assistance to humans who are attempting to prove or disprove mathematical theorems.
%
%Thus we have a situation in which formal methods researchers advocate for the use of formal methods due to the inherent uncertainties involved with empirical testing of systems. However, they are satisfied to leave validation of the human component, the most risky component, to empirical testing.

%Some researchers have sought to develop formal methods for mitigating human-induced sources of failure~\cite{ButlerModeConfusion,Rushby_Mode_Confusion}. Thus far this work has focused on the development of formal methods tools for analyzing human machine interaction during the design and specification phase, rather than at runtime. The goals have been to develop software, and techniques for verifying the correctness of that software, to avoid mode confusion, a type of pilot error wherein the pilot believes the autopilot is in one mode, when in reality it is in another. In these situations, the autopilot is not offering protections concerning flight control inputs like thrust and pitch, so the pilot risks providing dangerous inputs.

%Butler, Miller, Potts, and Carreno~\cite{ButlerModeConfusion} trace mode confusion to three sources:
%\begin{enumerate}
%	\item poor display of automation state
%	\item unnecessarily complex automation
%	\item flight crew has an incorrect mental model of the state of the aircraft
%\end{enumerate}

%\noindent They say human factors research focuses on mitigating item (1) above, and they develop formal models in the PVS automated theorem prover to address items (2) and (3). However, the formal models they develop are of the automation system, not of the human components.

%Similarly, Rushby~\cite{Rushby_Mode_Confusion} describes a class of errors he calls automation surprises, which are distinct but related to mode confusion. These occur when a pilot becomes surprised by the automated behavior of the system. He proposes a formal method addressing automation surprises that constructs a model of the system behavior, constructs a formal specification of a possible pilot's mental model of the system, and compares them for disagreement. His solution focuses on identifying potential automation surprises in the system, and both models are of the system behavior itself: one directly of the model, and one indirectly of the model, mediated through a hypothetical pilot's mind.

%In each of the above cases, the formal efforts focus on modeling the system itself, rather than on the human component, and they address the problem at design time. This work complements theirs by constructing a formal model of the pilot herself, and addressing the problem at runtime. Similarly, this work aims to address a variety of problems due to a pilot's loss of situational awareness, including mode confusion. The next chapter introduces Dynamic Agent Safety Logic.